René Witte
Machine Learning
Primer
History
ML Types
Process
Clustering Documents
Motivation
k-Means Clustering
Application Example
Classifications &
Predictions
Introduction
Classification with kNN
Regression with kNN
Evaluation
Evaluation Methodology
Evaluation Metrics
Error Analysis
Overfitting
Underfitting
Cross-Validation
Notes and Further
Reading
6.1
Lecture 6
Machine Learning for Intelligent Systems
Introduction, Clustering, Classification, Regression, Evaluation
COMP 474/6741, Winter 2024
Department of Computer Science
and Software Engineering
Concordia University
6.2
Outline
1 Machine Learning Primer
2 Clustering Documents
3 Classifications & Predictions
4 Machine Learning Evaluation
5 Notes and Further Reading
6.3
AI, ML, DL
https://medium.com/machine-learning-for-humans/neural-networks-deep-learning-cdad8aeae49b
6.4
Learn from experience
In 1959, Arthur Samuel first proposed the concept
Machine Learning:
“A computer program is said to learn from expe-
rience E with respect to some class of tasks T
and performance measure P if its performance at
tasks in T, as measured by P, improves with expe-
rience E.”
6.5
Automated Reasoning
Inference
Process of deriving new facts from a set of premises
Types of logical inference
1 Deduction
2 Abduction
3 Induction
6.6
Deduction
aka Natural Deduction
• Conclusion follows necessary from the premises.
• From A⇒ B and A, we conclude that B
• We conclude from the general case to a specific example of the general case
• Example:
1 All men are mortal.
2 Socrates is a man.
3 from 1 ∧ 2 ⇒ Socrates is mortal.
• Our subclass inference in RDFS also falls into this category.
Aryan Saxena
6.7
Abduction
Abductive Reasoning
• Conclusion is one hypothetical (most probable) explanation for the premises
• From A⇒ B and B, we conclude A
1 Drunk people do not walk straight.
2 John does not walk straight.
3 from 1 ∧ 2 ⇒ John is drunk.
• Not sound. . . but may be most likely explanation for B
• Used in medicine. . .
1 in reality: disease⇒ symptoms
2 patient complains about some symptoms. . . doctor concludes a disease
6.8
Induction
Inductive Reasoning
• Conclusion about all members of a class from the examination of only a few
member of the class.
• From A∧C⇒ B and A∧D⇒ B, we conclude A⇒B
• We construct a general explanation based on specific cases
1 All CS students in COMP 474 are smart.
2 All CS students on vacation are smart.
3 from 1 ∧ 2 ⇒ All CS students are smart.
• Not sound
• But, can be seen as hypothesis construction or generalisation
6.9
Inductive Learning
Learning from examples
• Most work in ML
• Examples are given (positive and/or negative) to train a system in a
classification (or regression) task
• Extrapolate from the training set to make accurate predictions about future
examples
• Given a new instance X you have never seen, you must find an estimate of the
function f(X) where f(X) is the desired output
From datascience.com, https://towardsdatascience.com/cat- dog- or- elon- musk- 145658489730
https://towardsdatascience.com/cat-dog-or-elon-musk-145658489730
6.10
Example
• Given pairs (X , f (X )) (the training set – the data points)
• Find a function f that fits the training set well
• So that given a new X , you can predict its f (X ) value
Note: choosing one function over another beyond just looking at the training set is
called inductive bias (eg. prefer “smoother” functions)
6.11
Inductive Learning Framework
Feature Vectors
• Input data are represented by a vector of features, X
• Each vector X is a list of (attribute, value) pairs.
• Ex: X =[nose:big, teeth:big, eyes:big, moustache:no]
• The number of attributes is fixed (positive, finite)
• Each attribute has a fixed, finite number of possible values
• Each example can be interpreted as a point in a n-dimensional feature space,
where n is the number of attributes (features)
6.12
Some Machine Learning Techniques
Probabilistic Methods
• e.g., Naïve Bayes Classifier
Decision Trees
• Use only discriminating features as questions in a big if-then-else tree
Neural Networks
• Also called parallel distributed processing or connectionist systems
• Intelligence arise from having a large number of simple computational units
NB: Deep Learning ≈ Neural Networks “on steroids”
6.13
Supervised Learning
Labeled Data
In Supervised Learning, we train a system using data with known labels.
EDA = Exploratory Data Analysis, see https://towardsdatascience.com/exploratory-data-analysis-8fc1cb20fd15
https://towardsdatascience.com/exploratory-data-analysis-8fc1cb20fd15
6.14
Unsupervised Learning
Unlabeled Data
In Unsupervised Learning, we have only unlabeled data and train a system without
guidance from an expected output.
6.15
Reinforcement Learning
Feedback and Rewards
In Reinforcement Learning, an agent learns to make decisions by performing
actions and receiving feedback in the form of rewards or penalties, optimizing its
behavior towards long-term objectives.
6.16
AI Learns to Park
https://www.youtube.com/watch?v=VMp6pq6_QjI
6.17
Machine Learning Categories
6.18
http://www.cognub.com/index.php/cognitive-platform/
6.19
General machine learning process
→ Worksheet #5: Task 1
6.20
6.21
Math with Words
Vector Space Model
• A mathematical model to portray an n-dimensional space
• Entities are described by vectors with n coordinates in a real space Rn
• Given two vectors, we can compute a similarity coefficient between them
• Cosine of the angle between two vectors reflects their degree of similarity
tf = 1 + log(tft,d ) (1)
idf = log
N
dft
(2)
cos(~q , ~d ) =
∑|v|
i=1 qi · di√∑|v|
i=1 q i
2 ·
√∑|v|
i=1 d i
2
(3)
6.22
Intelligent Systems for Investigative Journalism
Organize large, unstructured document collections:
• Enron email dataset – ca. 500,000 emails from management
• Wikileaks – often releases millions of documents
• Guantanamo Bay Files, TPP Agreements, CIA Documents, German BND-NSA
Inquiry, . . .
• Facebook internal documents leaks (Cambridge Analytica scandal, 7000
documents)
• Luanda Leaks (715,000 emails, charts, contracts, audits, etc.)
• Paradise Papers (13.4 million confidential papers regarding offshore
investments)
6.23
https://www.thestar.com/news/paradise-papers/2019/01/29/canada-revenue-agency-launches-100-audits-after-parad
ise-papers-leak.html
https://www.thestar.com/news/paradise-papers/2019/01/29/canada-revenue-agency-launches-100-audits-after-paradise-papers-leak.html
6.24
https://opensemanticsearch.org
6.25
Clustering
• Remember, we do not “classify” documents (like in “spam vs. ham”)
• Rather, we group similar documents together
• Often used as a first exploratory step in data analysis
• Data points (here: documents) in individual clusters can be further analyzed,
possibly with different methods
6.26
What are Clusters?
• The organization of unlabeled data into similarity groups, called clusters
• A cluster is a collection of data items which are “similar” between them, and
“dissimilar” to data items in other clusters.
• Generally, there is no right or wrong answer to what the clusters in a dataset
are.
6.27
Clustering Techniques
6.28
Partition-based Clustering
K-means (MacQueen, 1967) is a partitional clustering algorithm:
• Given m vectors in an n-dimensional space, ~x1, . . . , ~xm ∈ Rn
• User defines k , the number of clusters
Algorithm
1 Pick k points from the dataset (usually at random).
These points represent our initial group centroïds.
2 Assign each data point ~xi to the nearest centroïd.
3 When all data points have been assigned, recalculate the positions of the k
centroïds as the average of the cluster.
4 Repeat Steps 2 and 3 until none of the data instances change group
(or changes stay below a given convergence limit ∆).
6.29
Euclidian Distance
To find the nearest centroïd:
• a possible metric is the
Euclidean distance
• distance d between 2 points p, q
p = (p1,p2, . . . ,pn)
q = (q1,q2, . . . ,qn)
d =
√√√√ n∑
i=1
(pi − qi )2
• where to assign a data point ~x?
• → for all k clusters, choose the one
where ~x has the smallest distance
6.30
Example (1/5)
2D-vectors, k=3: Initialize random centroïds
6.31
Example (2/5)
Partition data points to closest centroïds
6.32
Example (3/5)
Compute new centroïds
6.33
Example (4/5)
Re-assign data points to closest new centroïds
6.34
Example (5/5)
Repeat until clusters stabilize
6.35
k-Means Clustering Illustrated
https://www.youtube.com/watch?v=5I3Ei69I40s
→ Worksheet #5: Task 2
6.36
k-Means: Pros & Cons
Pros
• Simple, easy to understand and implement
• Converges very fast
• Efficient: Time complexity O(t · k · n), with
• n number of data points
• k number of clusters
• t number of iterations
→ considered linear for practical purposes
Cons
• User needs to choose k (usually not known)
• Sensitive to outliers
• Different results on same dataset, based on initial (random) centroïds
6.37
k-Means & Outliers
6.38
k-Means: Sensitivity to Initial Seeds
6.39
k-Means
Summary
• Despite weaknesses, k-means is still one of the most popular algorithms, due
to its simplicity and efficiency
• No clear evidence that any other clustering algorithm performs better in general
• Comparing different clustering algorithms is a difficult task:
No one knows the correct clusters!
6.40
Document Clustering Example: Analyzing NSF Research Grants
https://www.youtube.com/watch?v=85fZcK5EpnA
6.41
6.42
Classification of Data
6.43
Classification Algorithms
6.44
k-Nearest-Neighbor (kNN) Classification
kNN Algorithm
Training: only store feature vectors + class labels
Testing: Find the k data points nearest (e.g., Euclidian distance) to the new
value. Resulting class is decided by majority vote.
Note: in this simple form, kNN has no training effort, but large testing effort
(so-called lazy learning)
Copyright Antti Ajanki (https://commons.wikimedia.org/wiki/File:KnnClassification.svg), “KnnClassification”,
licensed under https://creativecommons.org/licenses/by- sa/3.0/legalcode
https://commons.wikimedia.org/wiki/File:KnnClassification.svg
https://creativecommons.org/licenses/by-sa/3.0/legalcode
6.45
kNN Classification
With k = 1
• Compute the distance of the unknown sample to all existing samples
• Assign the class of the closest neighbor to the new sample
• Distance can be computed with different metrics, e.g.,
Euclidean distance or Manhattan distance
Copyright 2017 by O’Reilly Media, Inc., [MG17]
6.46
kNN Classification: General case
With arbitrary k
• kNN classification becomes a voting algorithm
• assign the same class as the majority of the k closest neighbors to the new
sample
• Choice of k is dependent on data set
6.47
Netflix: Predict Success of Original Content
In 2013, Netflix decided to commission two seasons of the U.S. remake of the
British series House of Cards based on an analysis of its customers’ data
https://informationstrategyrsm.wordpress.com/2014/10/19/big-data-analytics-house-of-cards-and-future-of-television-c
reation-consumption/
→ Worksheet #5: Task 3
https://informationstrategyrsm.wordpress.com/2014/10/19/big-data-analytics-house-of-cards-and-future-of-television-creation-consumption/
6.48
Regression
Forecasting or predicting a value: e.g., house price, movie rating, temperature at noon, ...
6.49
kNN Regression
• Find the nearest existing data point to a new sample as before
• Assign the value of this point (e.g., price, rating, ...) to the new instance
• Note: given n-dimensional vectors, we are using n − 1 dimensions for the similarity
and the final for the predicted value
6.50
kNN Regression: General Case
Find the k nearest existing data points
Assign the average of their values to the new point
• Note that this algorithm cannot extrapolate
→ Worksheet #5: Task 4
6.51
Machine Learning at Netflix
https://www.youtube.com/watch?v=X9ZES-fsxgU
6.52
6.53
Evaluation of a ML Model
Methodology
• How do you know if what you learned is correct?
• You run your classifier on a data set of unseen examples (that you did not use
for training) for which you know the correct classification (“gold standard”)
Training vs. testing data
• Split data into training (80%) and testing (20%) sets
• Depending on the ML algorithm, the training set can be further split into:
• Actual training set (80%)
• Validation set (20%)
6.54
Standard Methodology
1 Collect a large set of examples (all with correct classifications)
2 Divide collection into training, validation and test set
3 Apply learning algorithm to training set to learn the parameters
4 Measure performance with the validation set, and adjust hyper-parameters to
improve performance
5 Performance not good enough? ⇒ 3
6 Measure performance with the test set
DO NOT LOOK AT THE TEST SET
until you arrived at Step 6.
Parameters
Basic values learned by the ML
model from data, e.g.:
• for NB: prior & conditional
probabilities
• for k-Means: centroids, cluster
assignments
• for ANNs: weights
Hyper-Parameters
Parameters used to set up the ML
model, e.g.:
• for NB: value of delta for
smoothing
• for kNN: value of k
• for ANNs: # of hidden layers,
# of nodes per layer. . .
6.55
Metrics
Accuracy
• % of instances of the test set the algorithm correctly classifies
• when all classes are equally important and represented
Recall & Precision
• when one class is more important than the others
F-Measure
• Combined Precision & Recall (harmonic mean)
6.56
ML Evaluation
Evaluation of Classifiers
What kind of errors can we make?
Reality says. . .
Positive Negative
Model predicts. . . Positive True Positive (TP) False Positive (FP)
Negative False Negative (FN) True Negative (TN)
This is a so-called (binary) confusion matrix
Error Types
• False positive classification: Type I error
(“convict the innocent!”)
• False negative classification: Type II error
(“free the guilty!”)
Important realization: not all errors are created equal!
Voltaire: “It is better to risk saving a guilty man than to condemn an innocent one.”
6.57
Commonly used
• Accuracy = (TP + TN)/(TP + TN + FP + FN)
• Recall = TP/(TP + FN)
• Precision = TP/(TP + FP)
• F1-score = 2·Precision·Recall
Precision+Recall (harmonic mean)
Mind the evaluation task
Precision, recall etc. are defined slightly differently
for:
• Information retrieval tasks
• Classification tasks
• Ranked retrieval tasks
• Information extraction tasks
→ Worksheet #5: Task 5
Copyright by Walber (https://commons.wikimedia.org/wiki/File:Precisionrecall.svg), licensed under the Creative
Commons Attribution-Share Alike 4.0 International license
https://creativecommons.org/licenses/by- sa/4.0/legalcode
https://commons.wikimedia.org/wiki/File:Precisionrecall.svg
https://creativecommons.org/licenses/by-sa/4.0/legalcode
6.58
Confusion Matrix
• Where did the learner go wrong ?
• Use a confusion matrix (contingency table)
6.59
Learning Curve
Copyright 2007–2019, scikit-learn developers (BSD License), https://scikit- learn.org/stable/auto_examples/model_selection/plot_learning_curve.html
Plot evaluation metric vs. size of training set
• the more, the better
• but after a while, not much improvement. . .
https://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html
6.60
Some Words on Training. . .
Watch out for:
• Noisy Data
• Overfitting
• Underfitting
6.61
Noisy Data
Common issues
• Two examples have the same feature-value pairs, but different outputs
• Some values of features are incorrect or missing (e.g., errors in the data
acquisition)
• Some relevant attributes are not taken into account in the data set
6.62
• If a large number of irrelevant
features are there, we may find
meaningless regularities in the data
that are particular to the training data
but irrelevant to the problem.
• Complicated boundaries overfit the
data (a.k.a. overtraining)
• they are too tuned to the particular
training data at hand
• They do not generalize well to the
new data
• Extreme case: “rote learning”
• Training error is low
• Testing error is high
Copyright by Chabacano (https://commons.wikimedia.org/wiki/File:Overfitting.svg) license under the Creative Commons Attribution-Share
Alike 4.0 International license, https://creativecommons.org/licenses/by- sa/4.0/legalcode
https://commons.wikimedia.org/wiki/File:Overfitting.svg
6.63
• We can also underfit data, i.e. use
too simple decision boundary
• Model is not expressive enough (not
enough features)
• a.k.a. Undertraining
• There is no way to fit a linear
decision boundary so that the
training examples are well separated
• Training error is high
6.64
Example: Animal Classification
Features
What about cat vs. dog?
[from: Alison Cawsey: The Essence of AI (1997)]
6.65
k-fold Cross-Validation
‘Re-use’ different parts of the training data for testing. E.g., 10-fold cross-validation:
• split data into 10 equal parts (optionally, randomly shuffle before)
• train on 9 of these, test on the 10th
• repeat 10 times, resulting in 10 different performance results
• average these for overall performance
Balancing Data Use and Model Validation
• Maximizes data use—every data point serves in both training (k − 1 times) and
testing (exactly once)
• Reduces risk of model overfitting by validating against multiple data subsets
• More robust performance estimate by averaging results across multiple splits
6.66
6.67
Reading Material
Required
• [MG17, Chapters 2, 3, 5] (kNN, k-Means, Evaluation)
Supplemental
• [PS12, Chapter 7] (ML Training)
• [PS12, Chapter 8] (Testing and Evaluation)
6.68
References
[MG17] Andreas C Müller and Sarah Guido.
Introduction to Machine Learning with Python.
O’Reilly, 2017.
https://concordiauniversity.on.worldcat.org/oclc/960211579.
[PS12] James Pustejovsky and Amber Stubbs.
Natural Language Annotation for Machine Learning.
O’Reilly, 2012.
https://concordiauniversity.on.worldcat.org/oclc/801812987.
https://concordiauniversity.on.worldcat.org/oclc/960211579
https://concordiauniversity.on.worldcat.org/oclc/801812987
Intro to ML
Machine Learning Primer
Classifications & Predictions
Machine Learning Evaluation
Notes and Further Reading
