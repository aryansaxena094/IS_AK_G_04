René Witte
Introduction
Neural Networks 101
Perceptron
Backpropagation
TensorFlow & Keras
Word Embeddings
Bag-of-Words Model
One-Hot Vectors
Word Embeddings with
Word2vec
Word Vectors with spaCy
fastText
Text Processing Neural
Network
Task and Data
Input Representation
Network Architecture
Applying the Trained Model
Document Vectors and
Vector Databases
Document Vectors with
Doc2vec
Notes and Further
Reading
9.1
Lecture 9
Neural Networks & Word Embeddings
COMP 474/6741, Winter 2024
Department of Computer Science
and Software Engineering
Concordia University
9.2
Outline
1 Introduction
2 Neural Networks 101
3 Word Embeddings
4 Hello, Text Processing Neural Network
5 Document Vectors and Vector Databases
6 Notes and Further Reading
9.3
Summary of Chatbot Approaches
Copyright 2019 by Manning Publications Co., [LHH19]
9.4
Generative Models
Example
Generate answers to analogy questions like:
“Man is to Woman what King is to _____?”
“Paris is to France as Tokyo is to _____?”
Today
• Introduction to Neural Networks
• Building word vectors (word embeddings)
• Math with word vectors
• Neural Networks for Text Processing using word embeddings
• Document Vectors and Vector Databases
→ Worksheet #8: Task 1
9.5
9.6
Say hello to one of your neurons
9.7
Basic Perceptron (Franz Rosenblatt, 1957)
9.8
Perceptron Details
Mathematical Perceptron
Input vector:
~x = [x0, x1, ..., xn]
Weights vector:
~w = [w0,w1, ...,wn]
Dot product:
~x · ~w =
n∑
0
wi · xi
Activation function:
f (~x) =
{
1, if ~x · ~w ≥ threshold
0, otherwise
The ‘bias’ unit & weight
• Bias: additional input that is always “1”
• Why? Consider the case that all xi = 0, but we need to output 1
• Notation differs in the literature, but idea is always the same
9.9
Perceptron vs. Biological Neuron
→ Worksheet #8: Task 2
9.10
Perceptron Learning
Learning the weights
Perceptron uses supervised learning:
• look at each training sample
• output correct?
• Yes: don’t change any weights
• No: update the weights that were activated
Updating the weights
Based on how much they contributed to the error:
• w ′i = wi + η · (label − predicted) · xi
(label: training example, predicted: calculated output)
• η is a hyperparameter called the learning rate (e.g., η = 0.2)
• Going through all training examples once is called an epoch
→ Worksheet #8: Task 3
9.11
Linearly Separable Data
9.12
Nonlinearly Separable Data
9.13
Perceptron Learning Rule
What can a single Perceptron learn?
• A single Perceptron can learn
linearly separable data
• Two dimensions: line,
three dimensions: plane, etc.
• It can not learn data that is not
linearly separable
• Example: the XOR function
This was pointed out in a famous book by
Minksy & Papert in 1969∗
So what, it’s useless?
Not quite. . . so far, we only used a single neuron.
• We can use a network of neurons to also learn non-linearly separable data!
∗[Marvin Minsky and Seymour Papert: Perceptrons: an introduction to computational geometry, MIT Press, 1969]
9.14
Multi-layer neural networks with hidden weights
9.15
Training multi-layer neural networks
• First proposed in 1969, but not used until 1980s because of high computational
demands
• Form of supervised learning like Perceptron training
• Basic idea like before: show input, compute output, determine error, and adjust
weights to reduce error
• Learning is done in two phases
• first, apply input and propagate forward until output layer is reached
• then, compute error and propagate backwards, adjusting weights until input layer is
reached
9.16
Forward step
Weighted input
Neurons in backpropagation networks compute the net weighted input like the
Perceptron:
X =
i=1
xiwi − θ
Activation function
But here we use a sigmoid activation function
Y sigmoid =
1
1 + e−X
9.17
Updating weights (I)
Cost function
Error between truth and prediction:
err(x) = |y − f (x)|
Cost function you want to minimize:
f (x) = min
err(xi)
Using mean squared error (MSE) cost function:
MSE(x) =
n
(yi − f (xi))
2
Typically used for classification tasks: Cross-Entropy
→ Worksheet #8: Task 4
9.18
Updating weights (II)
Backpropagation rule
• compute the gradient of the loss function with respect to the weights of the
network for a single input–output example
• iterating backwards from output layer to input layer, updating weights
• intuitively: minimize cost function representing the error of the network
• algorithm performs gradient descent to try minimizing the error
9.19
Convex Error Curve
9.20
Nonconvex Error Curve
9.24
Example Neural Network in Keras
from numpy import loadtxt
from keras.models import Sequential
from keras.layers import Dense
# load the dataset
dataset = loadtxt(’pima-indians-diabetes.data.csv’, delimiter=’,’)
# split into input (X) and output (y) variables
X = dataset[:,0:8]
y = dataset[:,8]
# define the keras model
model = Sequential()
model.add(Dense(12, input_dim=8, activation=’relu’))
model.add(Dense(8, activation=’relu’))
model.add(Dense(1, activation=’sigmoid’))
# compile the keras model
model.compile(loss=’binary_crossentropy’, optimizer=’adam’, metrics=[’accuracy’])
# fit the keras model on the dataset
model.fit(X, y, epochs=150, batch_size=10)
• Using the Pima Indians Diabetes dataset: predicting the onset of diabetes
based on diagnostic measures, like 2-Hour serum insulin (mu U/ml) and
Diastolic blood pressure (mm Hg)
See https://machinelearningmastery.com/tutorial-first-neural-network-python-keras/
https://machinelearningmastery.com/tutorial-first-neural-network-python-keras/
9.25
Output
Using TensorFlow backend.
Epoch 1/150
768/768 [==============================] - 0s 257us/step - loss: 4.7881 - accuracy: 0.6107
Epoch 2/150
768/768 [==============================] - 0s 87us/step - loss: 0.8344 - accuracy: 0.5964
Epoch 3/150
768/768 [==============================] - 0s 93us/step - loss: 0.7119 - accuracy: 0.6510
Epoch 4/150
768/768 [==============================] - 0s 87us/step - loss: 0.6776 - accuracy: 0.6484
Epoch 5/150
768/768 [==============================] - 0s 87us/step - loss: 0.6315 - accuracy: 0.6888
Epoch 6/150
768/768 [==============================] - 0s 84us/step - loss: 0.6358 - accuracy: 0.6602
Epoch 7/150
768/768 [==============================] - 0s 89us/step - loss: 0.6254 - accuracy: 0.6810
Epoch 8/150
768/768 [==============================] - 0s 85us/step - loss: 0.6086 - accuracy: 0.6615
Epoch 9/150
768/768 [==============================] - 0s 80us/step - loss: 0.6121 - accuracy: 0.6745
Epoch 10/150
768/768 [==============================] - 0s 80us/step - loss: 0.6072 - accuracy: 0.6745
...
Epoch 150/150
768/768 [==============================] - 0s 86us/step - loss: 0.5269 - accuracy: 0.7096
9.26
Word Embeddings with Word2vec
9.27
Bag-of-Words (BOW) Model
Task
Turn words into numbers, so we can feed them into a neural network.
9.28
Problems with the Bag-of-Words Model
Word order is ignored
Meaning of the text is lost
9.29
Vector dimensionality = Vocabulary size
With n-dimensional vectors of {0,1}, we can represent each word in our vocabulary
that has 1 (one) for the word, else 0 (zero).
We can encode the sentence The big dog as a series of three-dimensional vectors:
the big dog
1 0 0
0 1 0
0 0 1
(a “1” means on, or hot; a “0” means off, or absent.)
Note
• Unlike in the BOW model, we do not lose information
• Not practical for long documents
9.30
The ‘Curse of Dimensionality’
https://en.wikipedia.org/wiki/Curse_of_dimensionality
→ Worksheet #8: Task 5
9.31
Towards better ‘word vectors’
Word Vector Requirements
• Dense vectors (smaller dimensions, fewer zeroes)
• Capture semantics of words
• E.g., Animal-ness, Place-ness, Action-ness. . .
• The (cosine) distance between “cat” and “dog” should be smaller than between
“cat” and “house”
• Synonyms (e.g., “inflammable” and “flammable”) should have nearly identical word
vectors
Answer analogy questions
We could then use these vectors for semantic word math, e.g., to answer analogy
questions like:
“Who is to physics what Louis Pasteur is to germs?”
By calculating ~w(’Louis Pasteur’) −~w(’germs’) +~w(’physics’)
→ Worksheet #8: Task 6
9.32
Hand-crafting Word Vectors (6 words, 3 dimensions)
word_vector[’cat’] = .3*topic[’petness’] +
.1*topic[’animalness’] +
0*topic[’cityness’]
word_vector[’dog’] = .3*topic[’petness’] +
.1*topic[’animalness’] -
.1*topic[’cityness’]
word_vector[’apple’] = 0*topic[’petness’] -
.2*topic[’cityness’]
word_vector[’lion’] = 0*topic[’petness’] +
.5*topic[’animalness’] -
word_vector[’NYC’] = -.2*topic[’petness’] +
.5*topic[’cityness’]
word_vector[’love’] = .2*topic[’petness’] -
9.33
3D vectors for six words about pets and NYC
9.34
Automatic computation of word vectors
• In 2012, Thomas Mikolov (intern at Microsoft) trained a neural network to
predict word occurrences near each target word
• Released in 2013 (then working at Google) as Word2vec
• Word vectors (a.k.a. word embeddings) typically have 100-500 dimensions and
are trained on large corpora (e.g., Google’s 100 billion words news feed)
• Unsupervised learning (no manually labeled data required)
9.35
Geometry of Word2vec math
9.36
Computing the answer to the soccer team question
Finding word vectors near the result
• Result vector (with 100s of dimensions) is not going to match any other word
vector exactly
• Find closest results (e.g., using cosine similarity) for the answer
9.37
Word vectors for ten US cities projected onto a 2D map
9.38
Training a Word2vec model
Approaches
Skip-gram: predict the context of words (output words) from an input word
CBOW: (continuous-bag-of-words) predicts output word from nearby (input)
words
Using a pre-trained model
You can download pre-trained word embeddings for many domains:
• Google’s Word2vec model trained on Google News articles
• spaCy comes with word vector models (shown later)
• Facebook’s fastText model (for 294 languages)
• Various models trained on medical documents, Harry Potter, LOTR, ...
9.39
Training input and output example for the skip-gram approach
Skip-gram
• Skip-gram is an n-gram with gaps
• Goal: predict surrounding window of words based on input word
9.40
Training: Ten 5-grams from the sentence about Monet
→ Worksheet #8: Task 7
9.41
Neural Network example for the skip-gram training (1/2)
→ Worksheet #8: Task 8
9.42
Softmax
Softmax function
The softmax function σ takes as input a vector of K real numbers, and normalizes it
into a probability distribution consisting of K probabilities proportional to the
exponentials of the input numbers:
σ(z)j =
ezj∑K
k=1 ezk
(e = Euler’s number ≈ 2.71828)
Softmax properties
• “normalizes” vector to a [0..1] interval, where all values add up to 1
• often used as activation function in the output layer of a neural network
→ Worksheet #8: Task 9
9.43
Neural Network example for the skip-gram training (2/2)
9.44
Conversion of one-hot vector to word vector
9.45
In other words...
Hidden weights are our word vectors
• We’re not actually using the neural network we trained
• We’re just using the weights as our word embeddings
• (that’s a common trick in using neural networks)
Why does this work?
• Two different words that have a similar meaning will have similar context words
appearing around them
• So the output vector for these different words have to be similar
• So the neural network has to learn weights for the hidden layer that map these
(different) input words to similar output vectors
• So we will get similar word vectors for words that have a different surface form,
but similar (or related) semantics
This does not solve the disambiguation problem: there will be one word vector for
“bank”, including both “river bank” and “financial bank” contexts.
9.46
Now what?
Now we can do math with word vectors:
king − man + woman = queen
Paris − France + Germany = Berlin
fish + music = bass
road − ocean + car = sailboat
desert − sand + suburbia = driveways
dorm − students = bachelor pad
barn − cows = garage
yeti − snow + economics = homo economicus
See https://graceavery.com/word2vec-fish-music-bass/ for more fun examples
https://graceavery.com/word2vec-fish-music-bass/
9.47
Continuous Bag Of Words (CBOW)
Idea
• Slide a rolling window across a sentence to select the surrounding words for
the target word
• All words within the sliding window are considered to be the content of the
CBOW
9.48
Training input and output example for the CBOW approach
9.49
Ten CBOW 5-grams from sentence about Monet
9.50
CBOW Word2vec network
9.51
Which one to use?
Pros & Cons
• Skip-gram approach works well with small corpora and rare terms (more
training data due to the network structure)
• CBOW shows higher accuracies for frequent words and is faster to train
9.52
Enhancements & Optimizations
Various Improvements
Frequent Bigrams: Pre-process the corpus and add frequent bigrams as terms
(e.g., “New York”, “Elvis Presley”)
Subsampling: Sample words according to their frequencies (no stop word removal
for words like “a”, “the”) – similar to idf in tf-idf
Negative sampling: To speed up training, don’t update all weights, but pick some
negative samples to decide which weights to update
9.53
Using Word Vectors with spaCy
import spacy
nlp = spacy.load("en_core_web_lg") # make sure to use larger model!
tokens = nlp("dog cat banana")
for token1 in tokens:
for token2 in tokens:
print(token1.text, token2.text, token1.similarity(token2))
dog dog 1.0
dog cat 0.80168545
dog banana 0.24327646
cat dog 0.80168545
cat cat 1.0
cat banana 0.2815437
banana dog 0.24327646
banana cat 0.2815437
banana banana 1.0
Training your own Word2vec model using gensim
9.55
Google News Word2vec 300-D vectors projected onto a 2D map using PCA
9.56
Word vectors can be biased
Your word vectors represent what is in your corpus:
>>> word_model.distance(’man’, ’nurse’)
0.7453
>>> word_model.distance(’woman’, ’nurse’)
0.5586
So an AI using these word vectors will now have a gender bias!
9.57
fasttext.cc
Idea: train on character n-grams, not on word n-grams:
• E.g., for “whisper”, we can generate the following 2-grams and 3-grams
wh, whi, hi, his, is, isp, sp, spe, pe, per, er
• We can now deal with unseen words, misspelled words, partial words, etc.
• Open source project by Meta (Facebook) research; pre-trained models for 294
languages from Abkhazian to Zulu
9.58
Input Representation using Word Vectors
9.59
The Task
Binary Text Classification
We want to classify incoming text as one of two classes, Technology or Food.
Training Data
texts = [
’Google search engine’,
’Apple mobile devices’,
’delicious Pizza food’,
’tasty Burger meal’,
’Python programming language’,
’Italian Pasta dish’
]
labels = [0, 0, 1, 1, 0, 1] # 0 for Technology, 1 for Food
9.60
Using pre-trained GloVe word vectors
Available from https://nlp.stanford.edu/projects/glove/
• We’ll use the smallest pre-trained model, glove.6B.50d.txt (400,000
50-dimensional word embeddings, trained on 6 Billion Tokens)
two 0.58289 0.36258 0.34065 0.36416 0.34337 0.79387 -0.9362 0.11432 -0.63005 -0.55524 -0.28706 -0.47143 -0.75673 0.63868
0.22479 -0.64652 -0.074314 -0.34903 -0.97285 -0.53981 0.015171 0.24479 0.62661 0.070447 -0.51629 -1.2004 0.3122 -0.44053
-0.29869 -0.56326 4.022 0.38463 -0.028468 0.068716 1.0746 0.48309 0.2475 0.22802 -0.35743 0.40392 -0.54738 0.15244 0.41
0.15702 0.0077935 -0.015106 -0.28653 -0.16158 -0.35169 -0.82555
more 0.87943 -0.11176 0.4338 -0.42919 0.41989 0.2183 -0.3674 -0.60889 -0.41072 0.4899 -0.4006 -0.50159 0.24187 -0.1564
0.67703 -0.021355 0.33676 0.35209 -0.24232 -1.0745 -0.13775 0.29949 0.44603 -0.14464 0.16625 -1.3699 -0.38233 -0.011387
0.38127 0.0 38097 4.3657 0.44172 0.34043 -0.35538 0.30073 -0.09223 -0.33221 0.37709 -0.29665 -0.30311 -0.49652 0.34285
0.77089 0.60848 0.15698 0.029356 -0.42687 0.37183 -0.71368 0.30175
Loading the word embeddings
embeddings_index = {}
with open(’glove.6B.50d.txt’, ’r’, encoding=’utf-8’) as f:
for line in f:
values = line.split()
word = values[0]
vector = np.asarray(values[1:], dtype=’float32’)
embeddings_index[word] = vector
https://nlp.stanford.edu/projects/glove/
9.61
Preparing the input for the neural network
Converting the input text
We simply average the word vectors of the input words in a text to get the input to
the neural network
• This is bag-of-words again (we ignore the word order), but it’s ok for a simple
classification task
• We will later see more sophisticated neural network architectures
In Python...
def get_average_vector(text):
# Split the text into words
words = text.split()
# Retrieve vectors for each word and calculate the average
vectors = np.array([embeddings_index.get(word.lower(),
np.zeros(50)) for word in words])
if len(vectors) == 0:
return np.zeros(50) # Return zero vector if no words found
return np.mean(vectors, axis=0)
9.62
Define the Network Architecture
Binary Classification Task
Input Layer: 50-dimensional word embeddings, so 50 input neurons
Hidden Layer: We use 30 hidden neurons
Output Layer: Single output neuron, sigmoid activation function
Note: For more than two classes, you would use one neuron for each class in the
output layer and apply the softmax activation function
Using Keras
model = Sequential([
Dense(32, activation=’relu’, input_dim=50),
Dense(1, activation=’sigmoid’)
])
9.63
Training the Network
Compiling and Training our Model
model.compile(optimizer=’adam’,
loss=’binary_crossentropy’,
metrics=[’accuracy’])
model.fit(X_train, np.array(labels), epochs=30, verbose=1)
Epoch 1/30
1/1 [==============================] - 0s 391ms/step - loss: 0.7393 - accuracy: 0.5000
Epoch 2/30
1/1 [==============================] - 0s 4ms/step - loss: 0.7169 - accuracy: 0.5000
Epoch 30/30
1/1 [==============================] - 0s 3ms/step - loss: 0.2776 - accuracy: 1.0000
1/1 [==============================] - 0s 33ms/step
In real systems, you would use a train/test/validation split:
• Training loop (computing loss) runs on training data
• Accuracy is computed on validation data
• Techniques like early stopping are used to avoid overfitting
9.64
Testing the Model on unseen Data
Testing Data and Prediction
new_texts = [’I like Spaghetti’, ’Tesla makes electric cars’]
new_X = np.array([get_average_vector(text) for text in new_texts])
predictions = model.predict(new_X)
print(predictions) # Closer to 0: Technology, closer to 1: Food
The result...
[[0.62762314]
[0.34521136]]
9.65
9.66
Doc2Vec Training
Paragraph Vector – Distributed Memory (PV-DM)
• PV-DM: Predicts next word in a text given context words and a paragraph ID.
• Paragraph Matrix: Embedding vector representing the document
• Word Vectors: Embeddings representing individual words
• Context Window: Fixed-size, sliding over text
• Concatenation/Averaging: Combines word vectors with paragraph vector
• Prediction: Output word predicted from combined vector
9.67
Vector Databases for (Document) Vectors
Vector Databases: Unlock the potential of your data, https://medium.com/@tenyks_blogger/vector- databases- unlock- the- potential- of- your- data- 5bd4950bd72
• Efficient storage of high-dimensional vectors from models like Doc2Vec
• Optimized for handling and querying vector data
• API and service integration for AI-driven applications
• Examples: FAISS (Meta), Annoy (Spotify), Weaviate, ...
https://medium.com/@tenyks_blogger/vector-databases-unlock-the-potential-of-your-data-5bd4950bd72
9.68
9.69
Reading Material
Required
• [LHH19, Chapters 5, 6] (Neural Networks, Word Vectors)
Supplemental
• Gidi Shperber, A gentle introduction to Doc2Vec,
https://medium.com/wisio/a-gentle-introduction-to-doc2vec-db3e8c0cce5e
• The Tenyks Blogger, Vector Databases: Unlock the potential of your data,
https://medium.com/@tenyks_blogger/vector-databases-unlock-the-potential-o
f-your-data-5bd4950bd72
9.70
References
[LHH19] Hobson Lane, Cole Howard, and Hannes Max Hapke.
Natural Language Processing in Action.
Manning Publications Co., 2019.
https://concordiauniversity.on.worldcat.org/oclc/1102387045.
https://concordiauniversity.on.worldcat.org/oclc/1102387045
Neural Networks
Introduction
Neural Networks 101
Perceptron
Backpropagation
TensorFlow & Keras
Word Embeddings
Bag-of-Words Model
One-Hot Vectors
Word Embeddings with Word2vec
Word Vectors with spaCy
fastText
Hello, Text Processing Neural Network
Task and Data
Input Representation using Word Vectors
Network Architecture
Applying the Trained Model
Document Vectors and Vector Databases
Document Vectors with Doc2vec
Vector Databases
Notes and Further Reading
