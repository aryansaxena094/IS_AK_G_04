COMP 6721: AI
Artificial Intelligence:
Convolutional Neural Network
(CNN) Architectures
portion of slides from:  Fei Fei Li
1
Today
1. Applications
2. Backbone Models
1. Serial Cascading
2. Serial/Parallel Cascading
3. Residual Connection
4. Depthwise/Separable Convolutions
5. Squeeze-Excitation
2
History of AI
3
How Computers Recognize Objects?
4
Question: Objects are anywhere in the scene (in any orientation,
color hue, perspectives, illumination, etc), so how can we
recognize them?
Answer: Learn a ton of features (millions) from the bottom up,
by learning convolutional filters rather than pre-computing them
Feature Invariance to Perturbation is Hard
5
Viewpoint Variation
(Perspective Geometry)
Scale Variation Deformation
Illumination Conditions Background Clutter Intra-Class Variation
ImageNet Large Scale Visual Recognition
Challenge (ILSVRC)
6
ImageNet-1K Class
Dog
7
8
ImageNet-1K Class ImageNet-20K Class
Serial Cascade: AlexNet
9
â€¢ 5 Conv and 3 FC layers
â€¢ ReLU Activation
â€¢ Training on Multiple GPUs (GTX 580 with 3GB memory)
â€¢ Response Normalization
â€¢ Overlapping Pooling
â€¢ Heavy data augmentation
â€¢ Image translation/horizontal reflection
â€¢ Altering intensities of RGB channels using PCA
10
C
o
n
v
lu
ti
â€¦
Width
(ğ‘¾)
Height
(ğ‘¯)
#Input Channel
(ğ‘µğ‘°)
(ğ‘¾â€²)
(ğ‘¯â€²)
#Output Channel
(ğ‘µğ‘¶)
ğ‘­ğ‘¶ : , : , ğ’‹ = à·
ğ’Š=ğŸ
ğ‘µğ‘°
ğ‘­ğ‘° : , : , ğ’Š âˆ— ğ‘²(: , : , ğ’Š, ğ’‹) + ğ’ƒğ’‹
ğ‘­ğ‘° âˆˆ â„ğ‘¯Ã—ğ‘¾Ã—ğ‘µğ‘° ğ‘­ğ‘¶ âˆˆ â„ğ‘¯â€²Ã—ğ‘¾â€²Ã—ğ‘µğ‘¶
ğ‘² âˆˆ â„ğ’‰Ã—ğ’˜Ã—ğ‘µğ‘°Ã—ğ‘µğ‘¶
Reminder from Convolution Feature mapping
11
â€¢ Feature mapping via cascaded convolutional layers
12
â€¢ AlexNet was the coming out party for CNNs in the computer
vision community. This was the first time a model performed
so well on a historically difficult ImageNet dataset.
â€¢ How did the learned kernels responses look like?
13
â€¢ Further analysis on AlexNet pretrained kernels (e.g. in 1st
layer) reveals that convolution kernels encode features in
different orientations, frequencies, and colors
14
15
Serial Cascade: Going Deeper with VGG
16
â€¢ Investigate the effect of the
convolutional network depth
â€¢ Great boost is achieved by increasing
#layers to 16-19
â€¢ Won ILSVRC2014 challenge
â€¢ Key design choice
â€¢ 3x3 kernel size
â€¢ Stack of conv layers w/o pooling
â€¢ Conv stride=1 (no skipping)
â€¢ ReLU activation
â€¢ 5 Max-pooling (x2 downsampling)
â€¢ 3 FC layers
â€¢ Later designs added Batch Normalization
17
18
19
20
21
22
23
Serial Cascade: Going Deeper with VGGâ€”
Training Phase
24
â€¢ Input training image: fixed size of 224x224 crop
â€¢ Images have varying size, so upscale to e.g. 384x(N>384)
â€¢ Random crop 224x224
â€¢ Standard augmentation: random flip and RGB shift
â€¢ SGD-Momentum (next lecture)
â€¢ Regularization: dropout and weight decay
â€¢ Fast convergence (74 training epochs)
â€¢ Initialization (some sort of transfer-learning)
â€¢ Deeper networks are prone to vanishing-gradients
â€¢ 11-layer net: random initialization from N(0;0.01)
â€¢ Deeper nets: Top & bottom layers initialized with 11-layer. Other
layers: random initialization
Testing Phase (ImageNet-1k)
25
â€¢ Evaluation on variable size images
â€¢ Testing on multiple 224x224 crops [AlexNet]
â€¢ Multiple scales are tested (256xN, 384xN, 512xN) and class score averaged
â€¢ Error decreases with depth
â€¢ Using multiple scales is important
â€¢ Multi-scale training outperforms single scale training =
â€¢ Multi-scale testing further improves the results
Serial/Parallel Cascade: Inception Net
26
â€¢ Multiple scales are encoded in parallel and cascaded to next
layers
Residual Connection: ResNet Architecture
27
â€¢ Is learning better networks as easy as stacking more layers?
â€¢ Obstacle: deeper networks are difficult to train because of the
notorious problem of vanishing/exploding gradients
â€¢ Early solutions were proposed by introducing
â€¢ normalized initialization
â€¢ intermediate normalization layer
â€¢ Still accuracy gets saturated with increasing depth and then degrades
rapidly (this is not caused by overfitting!)
â€¢ Adding more layers leads to higher training error
28
â€¢ Residual learning: instead of hoping each few stacked layers
directly fit a desired underling mapping, we explicitly let these
layers fit a residual mapping
â€¢ Define underlying forward mapping by H(x) := F(x) + x
â€¢ Why this should help?
29
â€¢ Corresponding gradient back-propagation:
ğœ•ğœ–
ğœ•ğ‘¥
=
ğœ•ğ»(ğ‘¥)
âˆ™
ğœ•ğ» ğ‘¥
ğœ•ğ¹
+ 1 =
+
â€¢ Gradient from output layer transfers directly to the input layer and avoids
vanishing
30
â€¢ Plain baseline is inspired by VGG nets
â€¢ Shortcuts introduced on plain baseline
â€¢ Same number of filters are used for the
same output feature map size
â€¢ If the feature map size is halved, the
number of filters is doubled
â€¢ Down-sampling by stride=2
â€¢ Network ends with global-average-pooling
â€¢ 1000-way FC layer with softmax
â€¢ ResNet34 has 3.6 BFLOPS (18% of
VGG19 i.e. 19.6 BFLOPS)
31
â€¢ Different ResNet architectures of ResNet18, ResNet34,
ResNet50, ResNet101, ResNet152
32
â€¢ ResNet performance on ImageNet
Depthwise/Separable Convolutions
33
â€¢ Keep input channel convolution separate from mapping
to output feature map S
e
p
-C
âˆ—
34
âˆ— âˆ—
35
âˆ— âˆ—âˆ—
36
âˆ— âˆ—âˆ— âˆ—
ğ‘¤3,1
37
Æ©
ğ‘¤2,1
ğ‘¤1,1
ğ‘¤ğ‘ğ¼,1
ğ‘¤3,2
38
S
ğ‘¤2,2
ğ‘¤1,2
ğ‘¤ğ‘ğ¼,2
to output feature map
ğ‘¤3,3
39
ğ‘¤2,3
ğ‘¤1,3
ğ‘¤ğ‘ğ¼,3
ğ‘¤3,ğ‘ğ‘‚
40
â€¦â€¦
ğ‘¤2,ğ‘ğ‘‚
ğ‘¤1,ğ‘ğ‘‚
ğ‘¤ğ‘ğ¼,ğ‘ğ‘‚
41
ğ‘­ğ‘° âˆˆ â„ğ‘¯Ã—ğ‘¾Ã—ğ‘µğ‘°
42
â€¦ â€¦
ğ‘­ğ‘° âˆˆ â„ğ‘¯Ã—ğ‘¾Ã—ğ‘µğ‘° à´¥ğ‘­ğ‘° âˆˆ â„ğ‘¯â€²Ã—ğ‘¾â€²Ã—ğ‘µğ‘°
à´¥ğ‘² âˆˆ â„ğ’‰Ã—ğ’˜Ã—ğŸÃ—ğ‘µğ‘°
43
â€¦ â€¦â€¦
ğ‘­ğ‘° âˆˆ â„ğ‘¯Ã—ğ‘¾Ã—ğ‘µğ‘° à´¥ğ‘­ğ‘° âˆˆ â„ğ‘¯â€²Ã—ğ‘¾â€²Ã—ğ‘µğ‘° ğ‘­ğ‘¶ âˆˆ â„ğ‘¯â€²Ã—ğ‘¾â€²Ã—ğ‘µğ‘¶
à´¥ğ‘² âˆˆ â„ğŸÃ—ğŸÃ—ğ‘µğ‘°Ã—ğ‘µğ‘¶
44
â€¢ What are the benefits?
â€¢ Standard convolution has the computation cost of
â€¢ Depthwise-separable convolution has the computation cost of
â€¢ Reduction in computation ratio
ğ‘¶ ğ’‰ âˆ™ ğ’˜ âˆ™ ğ‘µğ‘° âˆ™ ğ‘µğ‘¶ âˆ™ ğ‘¯ âˆ™ ğ‘¾
Convolution
Kernel
Input
Feature Map
ğ‘¶ ğ’‰ âˆ™ ğ’˜ âˆ™ ğ‘µğ‘° âˆ™ ğ‘¯ âˆ™ ğ‘¾ + ğ‘µğ‘° âˆ™ ğ‘µğ‘¶ âˆ™ ğ‘¯ âˆ™ ğ‘¾
Conv
Kernel-1
Feature
Kernel-2
ğ’“ =
ğ’‰ğ’˜ğ‘µğ‘°ğ‘¯ğ‘¾ + ğ‘µğ‘°ğ‘µğ‘¶ğ‘¯ğ‘¾
ğ’‰ğ’˜ğ‘µğ‘°ğ‘µğ‘¶ğ‘¯ğ‘¾
ğŸ
ğ‘µğ‘¶
ğ’‰ğ’˜
ğ’‰
ğ’“
45
â€¢ Activation and Batch-Normalization are used in between
â€¢ 1x1 convolution also referred by Pointwise Convolution
MobileNet by Depthwise Separable
Convolutions
46
â€¢ All layers are followed
by BN and ReLU
â€¢ MobileNet has 28
separate layers
47
â€¢ Significant parameter reduction while maintaining performance
accuracy
Squeeze-Excitation Block
48
â€¢ The idea is to boost the representation power of the network
â€¢ SE: channel relationships are adaptively recalibrated in
channel-wise feature response by explicit modelling of
interdependencies between channels
49
50
Slide 1
Slide 2
Slide 3
Slide 4
Slide 5
Slide 6
Slide 7
Slide 8
Slide 9
Slide 10
Slide 11
Slide 12
Slide 13
Slide 14
Slide 15
Slide 16
Slide 17
Slide 18
Slide 19
Slide 20
Slide 21
Slide 22
Slide 23
Slide 24
Slide 25
Slide 26
Slide 27
Slide 28
Slide 29
Slide 30
Slide 31
Slide 32
Slide 33
Slide 34
Slide 35
Slide 36
Slide 37
Slide 38
Slide 39
Slide 40
Slide 41
Slide 42
Slide 43
Slide 44
Slide 45
Slide 46
Slide 47
Slide 48
Slide 49
Slide 50
