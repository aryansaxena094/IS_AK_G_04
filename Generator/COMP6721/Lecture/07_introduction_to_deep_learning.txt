COMP 6721: AI
Artificial Intelligence:
Introduction to Deep Learning
portion of slides from:  Y. Bengio, A. Ng and Y. LeCun, Fei Fei Li
1
Today
1. Motivation
2. Feature Learning
3. Building Block Design of CNN
o Convolutional Layer
o Activation Function
o Pooling Layer
o Normalization Layer
2
History of AI
3
What is Really Deep Learning?
1. Good old Neural Networks, with more layers/modules
2. Non-linear, hierarchical, abstract representations of data
3. Flexible models with any input/output type and size
4. Differentiable Functional Programming
4
The Google “Inception” deep neural network architecture for image recognition (27 layers)
Why Deep Learning Now?
1. Better algorithms & understanding
2. Computing power (GPUs, TPUs, ...)
3. Data with labels
4. Open-source tools and models
5
DLToday: Speech-to-Text
6
DLToday: Vision
7
8
DLToday: NLP
9
10
DLToday: Vision+NLP
11
DLToday: Image Translation
12
DLToday: Generative Models
13
14
Guess which one is generated?
Tacotron 2 Natural TTS Synthesis by Conditioning WaveNet on
Mel Spectrogram Predictions, 2017
DLToday: Language/Image Models
15
Open-AI GPT-3, or DALL-E: https://openai.com/blog/dall-e/
https://openai.com/blog/dall-e/
DLToday: Genomics
16
AlphaFold by DeepMind
https://deepmind.com/blog/article/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology
17
18
Feature (Encoder) Learning
E
n
c
o
d
e
r
FC
𝑐1
𝑐2
.
𝑐𝑀
Learnable Parameters
(Encoder Weights)
(Classifier Weights)
19
Encoder: Convolution Layer
Higher level of feature abstraction
20
Encoder: Cascade Layer Design
21
22
23
24
25
26
27
28
29
30
31
Y1: Square
Y2: Triangle
Y3: Circle
yN: Dimond
32
Encoder: Overall Training
1. Initialize all learnable parameters (e.g. random sampling)
2. Preprocess train image dataset and randomly shuffle them
3. Feed-forward images in mini-batches and compute “Target Predictions”
4. Calculate Error = Abs(“Target Labels” –  “Target Predictions”)
5. Propagate back error gradients and adjust weights using an
optimization method (e.g. stochastic gradient descent)
6. Repeat (1)-(4) to converge to a minimal error
Initial Drawbacks
1. Standard backpropagation with sigmoid activation function
does not scale well with multiple layers
❑ Weight of early layers change too slowly (no learning)
2. Overfitting
❑ Large network -> lots of parameters -> increased capacity to
“learn by heart”
3. Multilayered ANNs need lots of labeled data
❑ Most data is not labeled :(
33
Initial Drawbacks (1)
1. Standard gradient-based backpropagation does not scale well with
multiple layers…
When we multiply the gradients many times (for
each layer),  it can lead to …
a) Vanishing gradient problem:
❑ gradients shrink exponentially with the
number of layers
❑ so weight updates get smaller and smaller
❑ and weights of early layers change very
slowly and network learns very very slowly
b) Exploding gradient problem:
❑ multiplying gradients could also make them
grow exponentially.
❑ so weight updates get larger and larger
❑ and the weights can become so large as to
overflow and result in NaN values
34
https://medium.com/towards-data-science/activation-functions-and-its-types-which-is-better-a9a5310cc8f
https://medium.com/towards-data-science/activation-functions-neural-networks-1cbd9f8d91d6
To help, we can :
a) Use other activation
functions…
a) Do “gradient clipping”
(i.e. set bounds on the
gradients)
35
Initial Drawbacks (2)
https://www.kdnuggets.com/2015/04/preventing-overfitting-neural-networks.html
❑ Large network -> lots of parameters ->
increased capacity to “learn by heart”
❑ Solutions:
❑ Regularization:
❑ modify the error function that we minimize to penalize large weights.
❑ where f(w) grows larger as the weights grow larger and λ is the
regularization strength
❑ Dropout:
❑ keep a neuron active with some probability p or setting it to
zero otherwise.
❑ prevents the network from becoming too dependent on any
one neuron.
36
Initial Drawbacks (3)
3. Multilayered ANNs need lots of labeled data for training. To
solve the problem:
▪ Transfer Learning: use pre-trained encoder weights
▪ Fine-Tuning
37
Pre-trained weight initialization
▪ Deep-Tuning
38
▪ Image Augmentation: randomly perturb representation
▪ Shifting
▪ Flipping
▪ Rotation
▪ Skewing
▪ Color/illumination perturbation
39
Classic ML
Input
Motorbikes
“Non”-Motorbikes
Learning
algorithm
Feature 1
Feature 2
Manual Extraction of Features
(eg. edge detection, colors,
texture,…)
Classic ML,
requires labeled
data and hand-
crafted features
1. Needs expert
knowledge
2. Time-consuming
and expensive
3. Does not
generalize to
other domains
Slide from Y. LeCun
40
Automatic Feature Learning
Automatic
Feature
Representation
“wheel”
“handle”
handle
wheel
eg. handle, wheel, … With Automatic
Feature Learning:
1. We feed the
network the raw data
(not feature-
curated)
2. The features are
learned by the
network
3. Features learned
can be re-used in
similar tasks.
41
https://www.strong.io/blog-images/movie-posters/Slide6.png 42
❑ Each layer learns more abstract features that are then combined /
composed into higher-level features automatically
❑ Like the human brain …
❑ has many layers of neurons which act as feature detectors
❑ detecting more and more abstract features as you go up
❑ E.g. to classify an image of a cat:
▪ Bottom Layers: Edge detectors, curves, corners straight lines
▪ Middle Layers: Fur patterns, eyes, ears
▪ Higher Layers: Body, head, legs
▪ Top Layer: Cat or Dog
Deep Learning = Machine learning algorithms based on
learning multiple levels of representation / abstraction.
– Y. Bengio
43
44
What Types of Features?
▪ For image recognition
❑ pixel -> edge -> texton -> motif -> part -> object
▪ For NLP
❑ character -> word ->  constituents -> clause -> sentence -> discourse
▪ For speech:
❑ sample → spectral band -> sound -> … phone -> phoneme -> word
Figure from Y LeCun 45
Eg: Learning Image Features
Faces Cars Elephants Chairs
Actual images
(pixels)
46
Learned
object
parts
edges
Examples of learned objects parts from object categories
objects
Learned features /
representations can be used in
variety of OTHER classification
tasks… → deep learning
47
48
Building Block Design of CNN
A typical backbone
architecture design of CNN
C
v
lu
ti
A
a
P
lin
g
N
rm
liz
…
49
Feature Responses
50
512
51
52
53
Conv Layer of CNN
54
Using Spatial Structure
55
56
Feature Extraction with Convolution
57
What is Convolution Operation?
• Elementwise multiplication and addition
58
59
Sliding Window for Convolution
60
61
62
63
Output Feature Map size in Convolution
64
65
66
67
68
69
Padding for Convolution
70
https://cs231n.github.io/convolutional-networks/
71
Convolutional Feature Mapping
Width
(𝑾)
72
Height
(𝑯)
73
#Input Channel
(𝑵𝑰)
𝑭𝑰 ∈ ℝ𝑯×𝑾×𝑵𝑰
74
(𝑾′)
(𝑯′)
#Output Channel
(𝑵𝑶)
𝑭𝑰 ∈ ℝ𝑯×𝑾×𝑵𝑰 𝑭𝑶 ∈ ℝ𝑯′×𝑾′×𝑵𝑶
75
∗
Ʃ
76
∗ ∗
77
∗ ∗ ∗
78
∗ ∗ ∗ ∗
79
80
81
82
83
Convolutional filters (aka kernels) can
be represented in Tensor format𝒊
𝒋
𝑲 ∈ ℝ𝒉×𝒘×𝑵𝑰×𝑵𝑶
84
Ex1. 3x3x64x128
Ex2. 5x5x64x128
85
86
𝑭𝑶 : , : , 𝒋 = ෍
𝒊=𝟏
𝑵𝑰
𝑭𝑰 : , : , 𝒊 ∗ 𝑲(: , : , 𝒊, 𝒋) + 𝒃𝒋
87
Ex1. Input   Feature Map: 56x56x64 -->
Output Feature Map: 56x56x128 (Stride=1)1. 56x56x64 --> 28x28x128
(Stride=2)
88
Output Feature Map: 28x28x128 (Stride=2)1. 56x56x64 --> 28x28x128
89
Non-Linear Activation
90
Pooling (i.e. down-sampling)
91
92
93
94
95
96
97
98
99
100
101
102
103
104
Batch Normalization
• BN is basically Whitening Transformation
• i.e. makes distribution close to standard normal
• To bring stochasticity, batch statistics are used
• We make BN layer to learn its scaling/bias adjusting factors
෠𝐹 =
𝐹 − 𝜇
𝜎 + 𝜖
𝜇=𝐸 𝐹 ,  𝜎 = 𝑉𝑎𝑟(𝐹)
𝐹 − ො𝜇
ො𝜎 + 𝜖
Ƹ𝜇 =
𝐵
෍
𝑘=1
𝐹𝑘 , ො𝜎 =
(𝐹𝑘 − Ƹ𝜇)2
෠𝐹 = 𝛾
+ 𝛽
Learnable
parameter: scaling
parameter: bias
105
Why Do We Use BN?
106
107
How BN Helps?
• BN reduces the internal covariate shift
108
Convolution
Activation
Pooling
Normalization
Slide 1
Slide 2
Slide 3
Slide 4
Slide 5
Slide 6
Slide 7
Slide 8
Slide 9
Slide 10
Slide 11
Slide 12
Slide 13
Slide 14
Slide 15
Slide 16
Slide 17
Slide 18
Slide 19
Slide 20
Slide 21
Slide 22
Slide 23
Slide 24
Slide 25
Slide 26
Slide 27
Slide 28
Slide 29
Slide 30
Slide 31
Slide 32
Slide 33
Slide 34
Slide 35
Slide 36
Slide 37
Slide 38
Slide 39
Slide 40
Slide 41
Slide 42
Slide 43
Slide 44
Slide 45
Slide 46
Slide 47
Slide 48
Slide 49
Slide 50
Slide 51
Slide 52
Slide 53
Slide 54
Slide 55
Slide 56
Slide 57
Slide 58
Slide 59
Slide 60
Slide 61
Slide 62
Slide 63
Slide 64
Slide 65
Slide 66
Slide 67
Slide 68
Slide 69
Slide 70
Slide 71
Slide 72
Slide 73
Slide 74
Slide 75
Slide 76
Slide 77
Slide 78
Slide 79
Slide 80
Slide 81
Slide 82
Slide 83
Slide 84
Slide 85
Slide 86
Slide 87
Slide 88
Slide 89
Slide 90
Slide 91
Slide 92
Slide 93
Slide 94
Slide 95
Slide 96
Slide 97
Slide 98
Slide 99
Slide 100
Slide 101
Slide 102
Slide 103
Slide 104
Slide 105
Slide 106
Slide 107
Slide 108
