COMP 6721: AI
Artificial Intelligence:
Optimization in Deep Learning
Some Slides from:  Goodfellow et al., Deep Learning
1
Today
1. Feedforward in Deep Learning
2. Backpropagation in Deep Learning
• Gradient Descent
• Stochastic Gradient Decent (SGD)
• Momentum SGD
• RMSProp
• ADAM
3. Scheduled Learning
4. Hyper-Parameter (HP) Tuning
2
History of AI
3
Feed-Forward in Deep Learning
4
E
n
c
o
d
e
r
FC
Learnable Params
(FC)
(CONV)
𝑐1
𝑐2
.
𝑐𝑀
5
𝜽𝑭𝑪
N    – Number of images for training
M    – Number of Class Images
c     – Confidence prediction score
𝜽 – Learnable parameter
𝒙 – Input Image
𝒚 – Prediction label
𝒚𝑮𝑻 – Ground-truth label 𝑦 =
, 𝑒. 𝑔. 𝑦𝑐𝑎𝑡 =
0.11
0.78
0.06
0.23
, 𝑦𝑐𝑎𝑡
𝐺𝑇 =
0
𝜽𝟏 𝜽𝟐 𝜽𝟑 𝜽𝟒 𝜽𝑳
𝒙𝟏𝒙𝟐𝒙𝟑
𝒙𝑵
6
N    – N
𝒙 ∈ ℝ𝑯×𝑾×𝟑 𝒚 ∈ ℝ𝑴
𝑭𝟏 ∈ ℝ
𝑯
𝟐×
𝑾
𝟐×𝟑𝟐
7
𝑭𝟐 ∈ ℝ
𝟒
×
×𝟔𝟒
8
𝑭𝟑 ∈ ℝ
𝟖
×𝟏𝟐𝟖
9
𝑭𝑳 ∈ ℝ𝟏×𝟏×𝟓𝟏𝟐
10
The output prediction label is generated by a function ‘f’ applied on input
image ‘x’ processed by learnable parameters
𝜽 = 𝜽𝟏, 𝜽𝟐, ⋯ , 𝜽𝑳, 𝜽𝑭𝑪
𝒇
𝒚 = 𝒇 𝒙; 𝜽
Back-Propagation in Deep Learning
11
• For each input image 𝒙𝒊 there is a corresponding ground-truth label 𝒚𝑮𝑻
which should be matched with output prediction label 𝒚
𝝐 = 𝑳 𝒚, 𝒚𝑮𝑻
𝑳: Loss-Function
Ideally Speaking: 𝝐 → 𝟎
12
• Calculate the gradient of loss prediction in terms of prediction label
𝝏𝝐
𝝏𝒚
=
𝝏𝑳 𝒚, 𝒚𝑮𝑻
13
14
• Back-propagate the gradient of loss-function into inner layers to calculate
the gradient of loss-function with respect to the learnable parameter of
that particular layer
𝝏𝑭𝑳
∙
𝑭𝑳
15
• We can now update parameter weights using gradient-descent method
𝜽𝑳
𝒌+𝟏 ← 𝜽𝑳
𝒌 − 𝜶 ∙
𝝏𝜽𝑳
Updated
Parameter
Previous
Error-
Gradient
Learning
Rate
16
• Updating on a single image sample introduces noisy gradient
direction and we can easily get stuck at local minima
• Select a mini-batch samples (from randomly shuffled data) and
average the gradients for updating
• aka we update not for every image but batch-of-images
• Superimpose all batch gradients to step into average direction
{𝒙𝟏, 𝒙𝟐, ⋯ , 𝒙𝑩}
𝜽𝒍
𝒌+𝟏 ← 𝜽𝒍
𝒌+𝟏 − 𝝆 ∙
𝟏
𝑩
෍
𝒊=𝟏
𝝏𝑭𝒊
𝒍
Stochastic
Error-Gradient
{
𝝏𝑭𝟏
,
𝝏𝑭𝟐
, ⋯ ,
𝝏𝑭𝑩
}
Stochastic Gradient Descent (SGD)
17
SGD with Momentum
18
• Learning with SGD can be sometimes slow
• Momentum approach we can be used to accelerate learning in the
face of exploring local minima of loss function
• High curvature
• Small but consistent gradient
• Noisy gradient
• Momentum approach accumulates an exponentially decaying moving
average of past gradients and continues to move in their direction
The contour lines depicts a quadratic loss
function with poor Hessian Matrix. The red
path cutting across the contour indicates
the path followed by momentum learning
rule to minimize the loss function
19
How to formulate it?
• Introduce a hyper-parameter (i.e. momentum) 𝛼 ∈ [0,1)
• 𝛼 determines how quickly the contributions of previous gradients
exponentially decay
• The update rule is given by
Momentum-SGD (MSGD)
20
21
Where to read from rest of topics?
22
Please refer to Reading Material as well as Class Discussions for the
rest of topics.
