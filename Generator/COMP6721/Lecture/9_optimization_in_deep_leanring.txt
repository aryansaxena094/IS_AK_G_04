COMP 6721: AI
Artificial Intelligence:
Optimization in Deep Learning
Some Slides from:  Goodfellow et al., Deep Learning
1
Today
1. Feedforward in Deep Learning
2. Backpropagation in Deep Learning
â€¢ Gradient Descent
â€¢ Stochastic Gradient Decent (SGD)
â€¢ Momentum SGD
â€¢ RMSProp
â€¢ ADAM
3. Scheduled Learning
4. Hyper-Parameter (HP) Tuning
2
History of AI
3
Feed-Forward in Deep Learning
4
E
n
c
o
d
e
r
FC
Learnable Params
(FC)
(CONV)
ğ‘1
ğ‘2
.
ğ‘ğ‘€
5
ğœ½ğ‘­ğ‘ª
N    â€“ Number of images for training
M    â€“ Number of Class Images
c     â€“ Confidence prediction score
ğœ½ â€“ Learnable parameter
ğ’™ â€“ Input Image
ğ’š â€“ Prediction label
ğ’šğ‘®ğ‘» â€“ Ground-truth label ğ‘¦ =
, ğ‘’. ğ‘”. ğ‘¦ğ‘ğ‘ğ‘¡ =
0.11
0.78
0.06
0.23
, ğ‘¦ğ‘ğ‘ğ‘¡
ğºğ‘‡ =
0
ğœ½ğŸ ğœ½ğŸ ğœ½ğŸ‘ ğœ½ğŸ’ ğœ½ğ‘³
ğ’™ğŸğ’™ğŸğ’™ğŸ‘
ğ’™ğ‘µ
6
N    â€“ N
ğ’™ âˆˆ â„ğ‘¯Ã—ğ‘¾Ã—ğŸ‘ ğ’š âˆˆ â„ğ‘´
ğ‘­ğŸ âˆˆ â„
ğ‘¯
ğŸÃ—
ğ‘¾
ğŸÃ—ğŸ‘ğŸ
7
ğ‘­ğŸ âˆˆ â„
ğŸ’
Ã—
Ã—ğŸ”ğŸ’
8
ğ‘­ğŸ‘ âˆˆ â„
ğŸ–
Ã—ğŸğŸğŸ–
9
ğ‘­ğ‘³ âˆˆ â„ğŸÃ—ğŸÃ—ğŸ“ğŸğŸ
10
The output prediction label is generated by a function â€˜fâ€™ applied on input
image â€˜xâ€™ processed by learnable parameters
ğœ½ = ğœ½ğŸ, ğœ½ğŸ, â‹¯ , ğœ½ğ‘³, ğœ½ğ‘­ğ‘ª
ğ’‡
ğ’š = ğ’‡ ğ’™; ğœ½
Back-Propagation in Deep Learning
11
â€¢ For each input image ğ’™ğ’Š there is a corresponding ground-truth label ğ’šğ‘®ğ‘»
which should be matched with output prediction label ğ’š
ğ = ğ‘³ ğ’š, ğ’šğ‘®ğ‘»
ğ‘³: Loss-Function
Ideally Speaking: ğ â†’ ğŸ
12
â€¢ Calculate the gradient of loss prediction in terms of prediction label
ğğ
ğğ’š
=
ğğ‘³ ğ’š, ğ’šğ‘®ğ‘»
13
14
â€¢ Back-propagate the gradient of loss-function into inner layers to calculate
the gradient of loss-function with respect to the learnable parameter of
that particular layer
ğğ‘­ğ‘³
âˆ™
ğ‘­ğ‘³
15
â€¢ We can now update parameter weights using gradient-descent method
ğœ½ğ‘³
ğ’Œ+ğŸ â† ğœ½ğ‘³
ğ’Œ âˆ’ ğœ¶ âˆ™
ğğœ½ğ‘³
Updated
Parameter
Previous
Error-
Gradient
Learning
Rate
16
â€¢ Updating on a single image sample introduces noisy gradient
direction and we can easily get stuck at local minima
â€¢ Select a mini-batch samples (from randomly shuffled data) and
average the gradients for updating
â€¢ aka we update not for every image but batch-of-images
â€¢ Superimpose all batch gradients to step into average direction
{ğ’™ğŸ, ğ’™ğŸ, â‹¯ , ğ’™ğ‘©}
ğœ½ğ’
ğ’Œ+ğŸ â† ğœ½ğ’
ğ’Œ+ğŸ âˆ’ ğ† âˆ™
ğŸ
ğ‘©
à·
ğ’Š=ğŸ
ğğ‘­ğ’Š
ğ’
Stochastic
Error-Gradient
{
ğğ‘­ğŸ
,
ğğ‘­ğŸ
, â‹¯ ,
ğğ‘­ğ‘©
}
Stochastic Gradient Descent (SGD)
17
SGD with Momentum
18
â€¢ Learning with SGD can be sometimes slow
â€¢ Momentum approach we can be used to accelerate learning in the
face of exploring local minima of loss function
â€¢ High curvature
â€¢ Small but consistent gradient
â€¢ Noisy gradient
â€¢ Momentum approach accumulates an exponentially decaying moving
average of past gradients and continues to move in their direction
The contour lines depicts a quadratic loss
function with poor Hessian Matrix. The red
path cutting across the contour indicates
the path followed by momentum learning
rule to minimize the loss function
19
How to formulate it?
â€¢ Introduce a hyper-parameter (i.e. momentum) ğ›¼ âˆˆ [0,1)
â€¢ ğ›¼ determines how quickly the contributions of previous gradients
exponentially decay
â€¢ The update rule is given by
Momentum-SGD (MSGD)
20
21
Where to read from rest of topics?
22
Please refer to Reading Material as well as Class Discussions for the
rest of topics.
