Artificial Intelligence:  Natural Language Processing
1
Artificial Intelligence:
Introduction to
Natural Language Processing
Menu
1. Introduction
2. Bag of word model
3. n-gram models
4. Linguistic features for NLP
‚óº 2
Languages
‚óº Artificial
‚ùë Smaller vocabulary
‚ùë Simple syntactic structures
‚ùë Non-ambiguous
‚ùë Not tolerant to errors (ex. Syntax error)
‚óº Natural
‚ùë Large and open vocabulary (new words everyday)
‚ùë Complex syntactic structures
‚ùë Very ambiguous
‚ùë Robust (ex. forgot a comma, a word‚Ä¶ still OK)
‚óº 3
Question Answering: IBM‚Äôs Watson
‚óº Won Jeopardy on February 16, 2011!
4
WILLIAM WILKINSON‚ÄôS
‚ÄúAN ACCOUNT OF THE PRINCIPALITIES OF
WALLACHIA AND MOLDOVIA‚Äù
INSPIRED THIS AUTHOR‚ÄôS
MOST FAMOUS NOVEL
Who is Bram
Stoker?
(Dracula)
Information Extraction
Subject: curriculum meeting
Date: January 15, 2012
To: Dan Jurafsky
Hi Dan, we‚Äôve now scheduled the curriculum meeting.
It will be in Gates 159 tomorrow from 10:00-11:30.
-Chris
Create new Calendar entry
Event: Curriculum mtg
Date: Jan-16-2012
Start:   10:00am
End:    11:30am
Where: Gates 159
Information Extraction & Sentiment Analysis
nice and compact to carry!
since the camera is small and light, I won't need to carry around
those heavy, bulky professional cameras either!
the camera feels flimsy, is plastic and very light in weight you have
to be very delicate in the handling of this camera
6
Size and weight
Attributes:
zoom
affordability
size and weight
flash
ease of use
‚úì
‚úó
slide from Olga Veksler (U. Western Ontario)
Machine Translation
Fully automatic
7
Helping human translators
Enter Source Text:
Translation from Stanford‚Äôs Phrasal:
Ëøô‰∏çËøáÊòØ‰∏Ä‰∏™Êó∂Èó¥ÁöÑÈóÆÈ¢ò .
This is only a matter of time.
Where we are today
Part-of-speech (POS) tagging
Named entity recognition (NER)
Sentiment analysis
mostly solved making good progress Good progress by
Deep Learning
Spam detection
Let‚Äôs go to Agra!
Buy V1AGRA ‚Ä¶
Colorless   green   ideas   sleep   furiously.
ADJ         ADJ    NOUN  VERB      ADV
Einstein met with UN officials in Princeton
PERSON              ORG                      LOC
Information extraction (IE)
You‚Äôre invited to our
dinner party, Friday May
27 at 8:30
Party
May
27
add
Best roast chicken in San Francisco!
The waiter ignored us for 20 minutes.
Machine translation (MT)
The 13th Shanghai International Film Festival‚Ä¶
Á¨¨13Â±ä‰∏äÊµ∑ÂõΩÈôÖÁîµÂΩ±ËäÇÂºÄÂπï‚Ä¶
Question answering (QA)
Q. How effective is ibuprofen in
reducing fever in patients with acute
febrile illness?
Parsing
I can see Alcatraz from the window!
Paraphrase
XYZ acquired ABC yesterday
ABC has been taken over by XYZ
Summarization
The Dow Jones is up
Housing prices rose
Economy
is goodThe S&P500 jumped
Coreference resolution
Carter told Mubarak he shouldn‚Äôt run again.
Word sense disambiguation (WSD)
I need new batteries for my mouse.
Dialog Where is Citizen Kane playing
in SF?
Castro Theatre at 7:30.
Do you want a ticket?
‚óº Because it is ambiguous:
1. The computer understands you
as well as your mother
understands you.
1. The computer understands that
you like (love) your mother.
as well as it understands your
mother.
Why is NLP hard?
‚ÄúAt last, a computer that understands you like your mother‚Äù
10
Another Example of Ambiguity
‚ùë Even simple sentences are highly ambiguous
‚ùë ‚ÄúGet the cat with the gloves‚Äù
11
And Even More Examples of Ambiguity
‚óº Iraqi Head Seeks Arms
‚óº Ban on Nude Dancing on Governor‚Äôs Desk
‚óº Juvenile Court to Try Shooting Defendant
‚óº Teacher Strikes Idle Kids
‚óº Kids Make Nutritious Snacks
‚óº British Left Waffles on Falkland Islands
‚óº Red Tape Holds Up New Bridges
‚óº Bush Wins on Budget, but More Lies Ahead
‚óº Hospitals are Sued by 7 Foot Doctors
‚óº Stolen Painting Found by Tree
‚óº Local HS Dropouts Cut in Half
‚óº Natural Language Processing
= automatic processing of written texts
1. Natural Language Understanding
‚ùë Input = text
2. Natural Language Generation
‚ùë Output = text
‚óº Speech Processing
= automatic processing of speech
1. Speech Recognition
‚ùë Input = acoustic signal
2. Speech Synthesis
‚ùë Output = acoustic signal
NLP vs Speech Processing
‚óº 12
Remember these slides?
13
The Ancient Land of NLP (aka GOFAI)
(circa A.D. 1950...mid 1980)
14
The Ancient Land of NLP
Speech
KingdomVillage of
CS &
Linguists
Information
Retrieval Forest
Machine
Learning
Island
Rule-based NLP
15
https://image.slidesharecdn.com/maryamsaihbani-161004094341/95/lefttoright-hierarchical-phrasebased-
translation-and-its-application-in-simultaneous-speech-translation-maryam-siahbani-4-638.jpg?cb=1477004310
Symbolic methods / Linguistic approach / Knowledge-rich approach
‚Ä¢ Cognitive approach
‚Ä¢ Rules are developed by hand in collaboration with linguists
1st Invasion of NLP, from ML
(mid 1980 ‚Äì circa 2010)
16
The Land of Statistical NLP
KingdomCity of
Statistical NLP
17
Syntactic
parsing
Part-of-
speech
tagging
stemming
tokenisation Decision trees
Statistical methods / Machine Learning / Knowledge-poor method
‚Ä¢ Engineering Approach
‚Ä¢ Rules are developed automatically (using machine learning)
‚Ä¢ But the linguistic features are hand-engineered and fed to the ML model
‚Ä¢ Applications: Information Retrieval, Predictive Text / Word Completion,
Language Identification, Text Classification, Authorship Attribution...
Neural networks
Na√Øve Bayes classifier
K-means clustering
Feature Extraction
(designed by hand)
Machine Learning
Model Applications
18
Applications
linguistic features are hand-engineered and fed to the ML model
2nd Invasion of NLP, by Deep Learning
(circa 2010-today)
19
The Modern Land of
Deep Language Processing
Kingdom
Metropolis
of  Deep
Language
Processing
Deep
20
Deep Neural Networks applied to NLP problems
‚Ä¢ And the linguistic features are found automatically!
‚óº 21
22
Bag-of-word Model (BOW)
‚óº A simple model where word order is ignored
‚óº used in many applications:
‚ùë NB spam filter seen in class a few weeks ago
‚ùë Information Retrieval (eg. google search)
‚ùë ...
‚óº But has severe limits to understand meaning of text...
‚óº Maybe we should take word order into account...
Word Freq.
Mary 2
apples 1
did 2
eat 1
John 1
kill 1
like 1
not 1
to 1
23
Limits of BOW Model
‚óº word order is ignored ==> meaning of text is lost.
‚óº n-grams take [a bit of] word order into account
Mary did kill John.
Mary did not like to eat
apples.
John did not kill Mary.
Mary did like to eat apples.
Mary did not like to kill
John.
Mary did eat apples.
...
‚óº 24
25
n-gram Model
‚óº An n-gram model is a probability distribution over sequences of
events (grams/units/items)
‚óº models the order of the events
‚óº Used when the past sequence of events is a good indicator of
the next event to occur in the sequence
‚óº i.e. To predict the next event in a sequence of event
‚óº E.g.:
‚ùë next move of player based on his/her past moves
‚óº left right right up ... up? down? left? right?
‚ùë next base pair based on past DNA sequence
‚óº AGCTTCG ... A? G? C? T?
‚ùë next word based on past words
‚óº Hi dear, how are ... helicopter? laptop? you? magic?
26
What‚Äôs a Language Model?
‚óº A Language model is a n-gram model over word/character
sequences
‚óº ie: events = words  or  events = character
‚óº P(‚ÄúI‚Äôd like a coffee with 2 sugars and milk‚Äù) ‚âà 0.001
‚óº P(‚ÄúI‚Äôd hike a toffee with 2 sugars and silk‚Äù) ‚âà 0.000000001
Applications of Language Models
‚óº Speech Recognition
‚óº Statistical Machine Translation
‚óº Language Identification
‚óº Spelling correction
‚ùë He is trying to fine out.
‚ùë He is trying to find out.
‚óº Optical character recognition / Handwriting
recognition
‚óº ‚Ä¶
In Speech Recognition
‚ñ™ Goal: find most likely sentence (S*) given the observed sound (O) ‚Ä¶
‚ñ™ ie. pick the sentence with the highest probability:
‚óº We can use Bayes rule to rewrite this as:
‚óº Since denominator is the same for each candidate S, we can ignore it for the
argmax:
Acoustic model --
Probability of the possible
phonemes in the language +
Probability of ‚â† pronunciations
Language model -- P(a sentence)
Probability of the candidate
sentence in the language
Given: Observed sound - O
Find: The most likely word/sentence ‚Äì S*
S1: How to recognize speech.  ?
S2: How to wreck a nice beach. ?
S3: ‚Ä¶
O)|P(SargmaxS*
LSÔÉé
=
P(S)S)|P(O argmaxS*
LS
ÔÇ¥=
ÔÉé
P(O)
S)P(S)|P(O
argmaxS*
29
argmax
P(acoustic signal)
P(acoustic signal | word sequence) x P(word sequence)
P(word sequence | acoustic signal)argmax
cewordsequen
ceword sequenc
Acoustic model
Language model
In Statistical Machine Translation
‚óº Assume we translate from fr[foreign] to English  i.e.: (en|fr)
Given: Foreign sentence - fr
Find: The most likely English
sentence ‚Äì en*
S1: Translate that!
S2: Translated this!
S3: Eat your soup!
S4‚Ä¶
Translation model
Automatic Language Identification‚Ä¶
guess how that‚Äôs done?
P(en) x en)|P(fr argmaxen*
en
31
‚ÄúShannon Game‚Äù (Shannon, 1951)
‚ÄúI am going to make a collect ‚Ä¶‚Äù
‚óº Predict the next word/character given the n-1 previous
words/characters.
https://en.wikipedia.org/wiki/Claude_Shannon
32
1st approximation
‚óº each word has an equal probability to follow any
other
‚ùë with 100,000 words, the probability of each word at any
given point is .00001
‚óº but some words are more frequent then others‚Ä¶
‚óº ‚Äúthe‚Äù appears many more times, than ‚Äúrabbit‚Äù
33
2nd approximation: unigrams
‚óº take into account the frequency of the word in
some training corpus
‚ùë at any given point, ‚Äúthe‚Äù is more probable than ‚Äúrabbit‚Äù
‚óº but does not take word order into account.  This
is the bag of word approach.
‚ùë ‚ÄúJust then, the white ‚Ä¶‚Äù
‚óº so the probability of a word also depends on the
previous words (the history)
P(wn |w1w2‚Ä¶wn-1)
34
n-grams
‚óº ‚Äúthe large green ______ .‚Äù
‚ùë ‚Äúmountain‚Äù? ‚Äútree‚Äù?
‚óº ‚ÄúSue swallowed the large green ______ .‚Äù
‚ùë ‚Äúpill‚Äù?  ‚Äúbroccoli‚Äù?
‚óº Knowing that Sue ‚Äúswallowed‚Äù helps narrow down
possibilities
‚óº i.e., going back 3 words before helps
‚óº But, how far back do we look?
35
Bigrams
‚óº first-order Markov models
‚óº N-by-N matrix of probabilities/frequencies
‚óº N = size of the vocabulary we are using
P(wn|wn-1)
1s
t
w
or
d
2nd word
a aardvark aardwolf aback ‚Ä¶ zoophyte zucchini
a 0 0 0 0 ‚Ä¶ 8 5
aardvark 0 0 0 0 ‚Ä¶ 0 0
aardwolf 0 0 0 0 ‚Ä¶ 0 0
aback 26 1 6 0 ‚Ä¶ 12 2
‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶
zoophyte 0 0 0 1 ‚Ä¶ 0 0
zucchini 0 0 0 3 ‚Ä¶ 0 0
36
Trigrams
‚óº second-order Markov models
‚óº N-by-N-by-N matrix of probabilities/frequencies
P(wn|wn-1wn-2)
3rd word
37
Why use only bi- or tri-grams?
‚óº Markov approximation is still costly
with a 20 000 word vocabulary:
‚ùë bigram needs to store 400 million parameters
‚ùë trigram needs to store 8 trillion parameters
‚ùë using a language model > trigram is impractical
38
Building n-gram Models
1. Data preparation:
‚ùë Decide on training corpus
‚ùë Clean and tokenize
‚ùë How do we deal with sentence boundaries?
‚óº I eat.  I sleep.
‚ùë (I eat) (eat I) (I sleep)
‚óº <s>I eat </s> <s> I sleep </s>
‚ùë (<s> I) (I eat) (eat </s>) (<s> I) (I sleep) (sleep </s>)
39
Example 1:
‚óº in a training corpus, we have 10 instances of
‚Äúcome across‚Äù
‚ùë 8 times, followed by ‚Äúas‚Äù
‚ùë 1 time, followed by ‚Äúmore‚Äù
‚ùë 1 time, followed by ‚Äúa‚Äù
‚óº so we have:
‚ùë
‚ùë P(more | come across) = 0.1
‚ùë P(a | come across) = 0.1
‚ùë P(X | come across) = 0  where X ‚â† ‚Äúas‚Äù, ‚Äúmore‚Äù, ‚Äúa‚Äù
8
across) C(come
as) across C(come
across) come |P(as ==
40
2. Count words and build model
‚ùë Let C(w1...wn) be the frequency of n-gram w1...wn
3. Smooth your model (see later)
)...wC(w
)...ww|(wP
1-n1
n1
1-n1n =
41
Example 2:
P(I want to eat British food)
= P(I|<s>) x P(want|I) x P(to|want) x P(eat|to) x P(British|eat) x P(food|British)
= .25       x .32           x .65             x .26          x .001                x .6
= .000008
P(on|eat) =   .16
P(some|eat) =  .06
P(British|eat) =  .001
‚Ä¶
P(I|<s>) =  .25
P(I‚Äôd|<s>) =   .06
P(want|I) =   .32
P(would|I) =  .29
P(don‚Äôt|I) =   .08
P(to|want) =  .65
P(a|want) =   .5
P(eat|to) =    .26
P(have|to) =    .14
P(spend|to)=    .09
P(food|British) =  .6
P(restaurant|British) = .15
Remember this slide...
42
43
Some Adjustments
‚óº product of probabilities‚Ä¶ numerical underflow
for long sentences
‚óº so instead of multiplying the probs, we add the
log of the probs
= log(P(I|<s>)) + log(P(want|I)) + log(P(to|want)) + log(P(eat|to)) +
log(P(British|eat)) + log(P(food|British))
= log(.25) + log(.32) + log(.65) + log (.26) + log(.001) + log(.6)
44
Problem: Data Sparseness
‚óº What if a sequence never appears in training corpus? P(X)=0
‚ùë ‚Äúcome across the men‚Äù --> prob = 0
‚ùë ‚Äúcome across some men‚Äù --> prob = 0
‚ùë ‚Äúcome across 3 men‚Äù --> prob = 0
‚óº The model assigns a probability of zero to unseen events ‚Ä¶
‚óº probability of an n-gram involving unseen words will be zero!
‚óº Solution: smoothing
‚ùë decrease the probability of previously seen events
‚ùë so that there is a little bit of probability mass left over for
previously unseen events
Remember this other slide...
45
46
Add-one Smoothing
‚óº Pretend we have seen every n-gram at least once
‚óº Intuitively:
‚ùë new_count(n-gram) = old_count(n-gram) + 1
‚óº The idea is to give a little bit of the probability
space to unseen events
47
Add-one: Example
unsmoothed bigram counts (frequencies):
‚óº Assume a vocabulary of 1616 (different) words
‚ùë V = {a, aardvark, aardwolf, aback, ‚Ä¶ , I, ‚Ä¶, want,‚Ä¶ to, ‚Ä¶, eat, Chinese, ‚Ä¶, food, ‚Ä¶, lunch, ‚Ä¶,
zoophyte, zucchini}
‚ùë |V| = 1616 words
‚óº And a total of N = 10,000 bigrams (~word instances) in the training corpus
I want to eat Chinese food lunch ‚Ä¶ Total
I 8 1087 0 13 0 0 0  C(I)=3437
want 3 0 786 0 6 8 6  C(want)=1215
to 3 0 10 860 3 0 12  C(to)=3256
eat 0 0 2 0 19 2 52  C(eat)=938
Chinese 2 0 0 0 0 120 1  C(Chinese)=213
food 19 0 17 0 0 0 0  C(food)=1506
lunch 4 0 0 0 0 1 0  C(lunch)=459
‚Ä¶         ...
N=10,000
48
unsmoothed bigram counts:
unsmoothed bigram conditional probabilities:
437 3
I)|P(I
000 10
P(II)
:note
49
Add-one, more formally
N: size of the corpus
i.e. nb of n-gram tokens in training corpus
B: number of "bins"
i.e. nb of different n-gram types
i.e. nb of cells in the matrix
e.g. for bigrams, it's (size of the vocabulary)2
B  N
1  )w w (w C
)w w (wP
n1 2
n21Add1
+
+ÔÇº
=ÔÇº
50
Add-one: Example (con‚Äôt)
add-one smoothed bigram counts:
add-one bigram conditional probabilities:
I 8   9 1087
1088
1 14 1 1 1  3437
C(I) + |V| = 5053
want 3  4 1 787 1 7 9 7  C(want) + |V| = 2831
to 4 1 11 861 4 1 13  C(to) + |V| = 4872
eat 1 1 23 1 20 3 53  C(eat) + |V| = 2554
Chinese 3 1 1 1 1 121 2  C(Chinese) + |V| = 1829
food 20 1 18 1 1 1 1  C(food) + |V| = 3122
lunch 5 1 1 1 1 2 1  C(lunch) + |V| = 2075
‚Ä¶         total = 10,000
N+|V|2 = 10,000 + (1616)2
= 2,621,456
I want to eat Chinese food lunch ‚Ä¶
I .0018
(9/5053)
.215 .00019 .0028
.00019 .00019 .00019
want .0014 .00035 .278 .00035 .0025 .0031 .00247
to .00082 .0002 .00226 .1767 .00082 .0002 .00267
eat .00039 .00039 .0009 .00039 .0078 .0012 .0208
51
Add-delta Smoothing
‚óº every previously unseen n-gram is given a low probability
‚óº but there are so many of them that too much probability mass is
given to unseen events
‚óº instead of adding 1, add some other (smaller) positive value ùõø
‚óº most widely used value for ùõø = 0.5
‚óº better than add-one, but still‚Ä¶
B   N
)w w (w C
n21AddD
ÔÅ§
52
Factors of Training Corpus
‚óº Size:
‚ùë the more, the better
‚ùë but after a while, not much improvement‚Ä¶
‚óº bigrams (characters) after 100‚Äôs million words
‚óº trigrams (characters) after some billions of words
‚óº Genre (adaptation):
‚ùë training on cooking recipes and testing on aircraft
maintenance manuals
53
Example: Language Identification
‚óº hypothesis: texts that resemble
each other (same author, same
language) share similar
character/word sequences
‚ùë In English character sequence
‚Äúing‚Äù is more probable than in
French
‚óº Training phase:
‚ùë construction of the language
model
‚ùë with pre-classified documents
(known language/author)
‚óº Testing phase:
‚ùë apply language model to unknown
text
54
‚óº bigram of characters
‚ùë characters = 26 letters (case insensitive)
‚ùë possible variations: case sensitivity,
punctuation, beginning/end of sentence
marker, ‚Ä¶
55
1. Train a character-based language model for Italian:
2. Train a character-based language model for Spanish:
3. Given a unknown sentence ‚Äúche bella cosa‚Äù  is it in Italian or in Spanish?
P(‚Äúche bella cosa‚Äù) with the Italian LM
P(‚Äúche bella cosa‚Äù) with the Spanish LM
4. Highest probability ‚Üí language of sentence
A B C D ‚Ä¶ Y Z
A 0.0014 0.0014 0.0014 0.0014 ‚Ä¶ 0.0014 0.0014
B 0.0014 0.0014 0.0014 0.0014 ‚Ä¶ 0.0014 0.0014
C 0.0014 0.0014 0.0014 0.0014 ‚Ä¶ 0.0014 0.0014
D 0.0042 0.0014 0.0014 0.0014 ‚Ä¶ 0.0014 0.0014
E 0.0097 0.0014 0.0014 0.0014 ‚Ä¶ 0.0014 0.0014
‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ 0.0014
Y 0.0014 0.0014 0.0014 0.0014 ‚Ä¶ 0.0014 0.0014
Z 0.0014 0.0014 0.0014 0.0014 0.0014 0.0014 0.0014
Google‚Äôs Web 1T 5-gram model
‚óº 5-grams
‚óº generated from 1 trillion words
‚óº 24 GB compressed
‚ùë Number of tokens: 1,024,908,267,229
‚ùë Number of sentences: 95,119,665,584
‚ùë Number of unigrams: 13,588,391
‚ùë Number of bigrams: 314,843,401
‚ùë Number of trigrams: 977,069,902
‚ùë Number of fourgrams: 1,313,818,354
‚ùë Number of fivegrams: 1,176,470,663
‚óº See discussion: http://googleresearch.blogspot.com/2006/08/all-our-n-gram-are-belong-
to-you.html
‚óº See Google Ngram Viewer: http://en.wikipedia.org/wiki/Google_Ngram_Viewer
56
http://googleresearch.blogspot.com/2006/08/all-our-n-gram-are-belong-to-you.html
http://en.wikipedia.org/wiki/Google_Ngram_Viewer
Problem with n-grams
‚óº Natural language is not linear ....
‚óº there may be long-distance dependencies.
‚ùë Syntactic dependencies
‚óº The man next to the large oak tree near ‚Ä¶ is tall.
‚óº The men next to the large oak tree near ‚Ä¶ are tall.
‚ùë Semantic dependencies
‚óº The bird next to the large oak tree near ‚Ä¶ flies rapidly.
‚óº The man next to the large oak tree near ‚Ä¶ talks rapidly.
‚ùë World knowledge
‚óº Michael Jackson, who was featured in ..., is buried in California.
‚óº Michael Bubl√©, who was featured in ..., is living in California.
‚óº More complex models of language are needed to handle such
dependencies.
57
‚óº 58
Linguistic features used for what?
59
60
Stages of NLU
source: Luger (2005)
61
Parsing (Syntax):
‚óº What words are available in a
language?  gfiioudd  / table
‚óº How to arrange words
together?
the rose is red / red the rose is
Semantic interpretation:
‚óº Lexical Semantics :
What is the
meaning/semantic
relations between
individual words?
Chair:  person?  Furniture?
‚óº Compositional
Semantics: What is the
meaning of phrases and
sentences?
The chair‚Äôs leg is broken
62
‚óº Discourse Analysis
How to relate the meaning of sentences to
surrounding sentences?
I have to go to the store.  I need butter.
I have to go to the university.  I need butter.
‚óº Pragmatics
How people use language in a social
environment?
Do you have a child?
Do you have a quarter?
‚óº World Knowledge
How knowledge about the world (history,
facts, ‚Ä¶) modifies our understanding of
text?
Bill Gates passed away last night.
63
64
Syntactic Parsing
1. Assign the right part of speech (NOUN, VERB, ‚Ä¶) to
individual words in a text
1. Determine how words are put together to form
correct sentences
‚óº The/DET rose/NOUN is/VERB red/ADJ.
‚óº Is/VERB red/ADJ the/DET rose/NOUN.
65
English Parts-of-Speech
‚óº Open (lexical) class words
‚ùë new words can be added easily
‚ùë nouns, main verbs, adjectives, adverbs
‚ùë some languages do not have all these categories
‚óº Closed (functional) class words
‚ùë generally function/grammatical words
‚ùë aka stop words
‚ùë ex. the, in, and, over, beyond‚Ä¶
‚ùë relatively fixed membership
‚ùë prepositions, determiners, pronouns, conjunctions, ‚Ä¶
Smurf talk on youtube:
https://www.youtube.com/watch?v=7BPx-vl8G00
66
Syntax
‚óº How parts-of-speech are organised into larger
syntactic constituents
‚óº Main Constituents:
‚ùë S: sentence The boy is happy.
‚ùë NP: noun phrase      the little boy from Paris, Sam Smith, I,
‚ùë VP: verb phrase eat an apple, sing, leave Paris in the night
‚ùë PP: prepositional phrase  in the morning, about my ticket
‚ùë AdjP: adjective phrase really funny, rather clear
‚ùë AdvP: adverb phrase slowly, really slowly
67
A Parse Tree
‚óº a tree representation of the application
of the grammar to a specific sentence.
68
a CFG consists of
‚óº set of non-terminal symbols
‚ùë constituents & parts-of-speech
‚ùë S, NP, VP, PP, D, N, V, ...
‚óº set of terminal symbols
‚ùë words & punctuation
‚ùë cat, mouse, nurses, eat, ...
‚óº a non-terminal designated as the starting symbol
‚ùë sentence S
‚óº a set of re-write rules
‚ùë having a single non-terminal on the LHS and one or more
terminal or non-terminal in the RHS
‚ùë S --> NP VP
‚ùë NP --> Pro
‚ùë NP --> PN
‚ùë NP --> D N
69
An Example
‚óº Lexicon:
N --> flight | trip | breeze | morning // noun
V --> is | prefer | like //
verb
Adj --> direct | cheapest | first //
adjective
Pro --> me | I | you | it //
pronoun
PN --> Chicago | United | Los Angeles // proper noun
D --> the | a | this //
determiner
Prep --> from | to | in //
preposition
Conj --> and | or | but //
conjunction
‚óº Grammar:
S --> NP VP // I +
prefer United
NP --> Pro | PN | D N | D Adj N   // I, Chicago, the morning
VP --> V | V NP | V NP PP // is, prefer +
United,
PP --> Prep NP //
to Chicago, to I ??
70
‚óº parsing:
‚ùë goal:
‚óº assign syntactic structures to a sentence
‚ùë result:
‚óº (set of) parse trees
‚óº we need:
‚ùë a grammar:
‚óº description of the language constructions
‚ùë a parsing strategy:
‚óº how the syntactic analysis are to be computed
71
Parsing Strategies
‚óº parsing is seen as a search problem
through the space of all possible parse
trees
‚ùë bottom-up (data-directed): words --> grammar
‚ùë top-down (goal-directed): grammar --> words
‚ùë breadth-first: compute all paths in parallel
‚ùë depth-first: exhaust 1 path before considering
another
‚ùë Heuristic search
72
Example: John ate the cat
‚óº Bottom-up parsing /
breadth first
1. John ate the cat
2. PN ate the cat
3. PN V the cat
4. PN V ART cat
5. PN V ART N
6. NP V ART N
7. NP V NP
8. NP VP
9. S
‚óº Top-down parsing /
depth first
1. S
2. NP VP
3. PN VP
4. John VP
5. John V NP
6. John ate NP
7. John ate ART N
8. John ate the N
9. John ate the cat
73
Depth-first vs Breadth-first
the cat eats the mouse.
‚óº depth-first: exhaust 1 path
before considering another
‚óº breadth-first:
‚ùë compute 1 level at a time
‚óº Heuristic search:
‚ùë e.g. preference to shorter rules
Grammar:
(1) S --> NP VP
(2) S --> VP
(3) S --> Aux NP VP
(4) NP --> Det N PP
(5) NP -- > Det N
(6) PP -- > Prep N
Lexicon:
(10) Det --> the
(11) N --> cat
(12) VB --> eats
S
NP-VP       VP   Aux-NP-VP
Det-N-PP Det-N ‚Ä¶
the  cat Prep-NP
74
Summary of Parsing Strategies
Depth
First
Breath
Heuristic
Search
Top down ‚úì ‚úì ‚úì
Bottom up ‚úì ‚úì ‚úì
75
Problem: Multiple parses
‚óº Many possible parses for a single sentence happens
very often‚Ä¶
‚ùë Prepositional phrase attachment (PP-attachment)
‚óº We painted the wall with cracks.
‚óº The man saw the boy with the telescope.
‚óº I shot an elephant in my pyjamas.
‚ùë Conjunctions and appositives
‚óº Maddy, my dog, and Samy
-- > (Maddy, my dog), and (Samy)
-- > (Maddy), (my dog), and (Samy)
‚óº These phenomena can quickly increase the number of
possible parse trees!
76
PP attachment:
The man saw the boy with the telescope.
Correct parse 1 Correct parse 2
source: Robert Dale.
77
Probabilistic Parsing
‚ÄúOne morning I shot an elephant in my pyjamas.  How he got
into my pyjamas, I don‚Äôt know.‚Äù
G. Marx, Animal Crackers, 1930.
‚óº Sentences can be very ambiguous‚Ä¶
‚ùë A non-probabilistic parser may find a large set of possible
parses
‚ùë --> need to pick the most probable parse one from the set
78
Example of a PCFG
‚óº Intuitively, P(VP ‚Üí V NP) is:
‚ùë the probability of expanding VP by a V NP, as opposed
to any other rules for VP
‚óº So for:
‚ùë VP: ‚àÄi ‚àëi P(VP --> B) = .7 + .3 = 1
‚ùë NP: ‚àÄi ‚àëi P(NP --> B) = .4 + .1 + .18 + .04 + .18 + .1 = 1
source:  Manning, and Sch√ºtze, Foundations of Statistical Natural Language Processing, MIT Press (1999)
79
‚óº Product of the probabilities of the rules used in subtrees
‚óº Ex: ‚ÄúAstronomers saw stars with ears.‚Äù
.
P(t1) = 1x.1x.7x1x.4x.18x1x1x.18 P(t2) = 1x.1x.3x.7x1x1x.18x1x.18
= .0009072 = .0006804
Probability of a parse tree
80
81
Semantic Interpretation
‚óº Map sentences to some representation of its
meaning
‚ùë e.g., logics, knowledge graph, embedding‚Ä¶
1. Lexical Semantics
‚óº i.e., Meaning of individual words
1. Compositional Semantics
‚óº i.e., Meaning of combination of words
82
Lexical Semantics
‚óº ie. The meaning of individual words
‚ùë A word may denote different things (ex. chair)
‚ùë The meaning/sense of words is not clear-cut
‚ùë E.g. Overlapping of word senses across languages
leg
patte
√©tape
jambe pied
animal
journey
human
chair
83
Word Sense Disambiguation (WSD)
‚óº Determining which sense of a word is used
in a specific sentence
‚ùë I went to the bank of Montreal and deposited 50$.
‚ùë I went to the bank of the river and dangled my feet.
84
‚óº WSD can be viewed as typical classification
problem
‚ùë use machine learning techniques (ex. Na√Øve Bayes
classifier, decision tree) to train a system
‚ùë that learns a classifier (a function f) to assign to
unseen examples one of a fixed number of senses
(categories)
‚óº Input:
‚ùë Target word: The word to be disambiguated
‚ùë Features?
‚óº Output:
‚ùë Most likely sense of the word
WSD as a Classification Problem
85
Features for WSD
‚óº intuition:
‚ùë sense of a word depends on the sense of surrounding words
‚óº ex: bass = fish, musical instrument, ...
‚óº So use a window of words around the target word as
features
Surrounding words Most probable sense
‚Ä¶river‚Ä¶ fish
‚Ä¶violin‚Ä¶ instrument
‚Ä¶salmon‚Ä¶ fish
‚Ä¶play‚Ä¶ instrument
‚Ä¶player‚Ä¶ instrument
‚Ä¶striped‚Ä¶ fish
86
‚óº Take a window of n words around the target word
‚óº Encode information about the words around the target word
‚ùë An electric guitar and bass player stand off to one side,
not really part of the scene, just as a sort of nod to gringo
expectations perhaps.
87
Na√Øve Bayes WSD
‚óº Goal: choose the most probable sense s* for a word given a vector
V of surrounding words
‚óº Feature vector V contains:
‚ùë Features: words [fishing, big, sound, player, fly, rod, ‚Ä¶]
‚ùë Value: frequency of these words in a window before & after the
target word [0, 0, 0, 2, 1, 0, ‚Ä¶]
‚óº Bayes decision rule:
‚ùë s* = argmaxsk P(sk|V)
‚ùë where:
‚óº S is the set of possible senses for the target word
‚óº sk is a sense in S
‚óº V is the feature vector
88
‚óº Training a Na√Øve Bayes classifier
= estimating P(vj|sk) and P(sk) from a sense-tagged training
corpus
= finding the most likely sense k
Nb of occurrences of feature j
over the total nb of features
appearing in windows of Sk
Nb of  occurrences of sense k
over nb of all occurrences of
ambiguous word
ÔÉ∑
ÔÉ∏
ÔÉ∂
ÔÉß
ÔÉ®
ÔÉ¶
ÔÉ•+=
n
1j
kjk
s
)s|P(v log  )P(s logargmaxs*
k
)s,count(v
)s|P(v
kt
kj
ÔÉ•
)count(word
)count(s
)P(s k
k =
89
Example
‚óº Training corpus (context window = ¬±3 words):
‚Ä¶Today the World Bank/BANK1 and partners are calling for greater relief‚Ä¶
‚Ä¶Welcome to the Bank/BANK1 of America the nation's leading financial institution‚Ä¶
‚Ä¶Welcome to America's Job Bank/BANK1 Visit our site and‚Ä¶
‚Ä¶Web site of the European Central Bank/BANK1 located in Frankfurt‚Ä¶
‚Ä¶The Asian Development Bank/BANK1 ADB a multilateral development finance‚Ä¶
‚Ä¶lounging against verdant banks/BANK2 carving out the...
‚Ä¶for swimming, had warned her off the banks/BANK2 of the Potomac. Nobody...
‚óº Training:
‚ùë P(the|BANK1) = 5/30 P(the|BANK2) = 3/12
‚ùë P(world|BANK1) = 1/30 P(world|BANK2) = 0/12
‚ùë P(and|BANK1) = 1/30 P(and|BANK2) = 0/12
‚ùë ‚Ä¶ ‚Ä¶
‚ùë P(off|BANK1) = 0/30 P(off|BANK2) =
1/12
‚ùë P(Potomac|BANK1) = 0/30 P(Potomac|BANK2) = 1/12
‚ùë P(BANK1) = 5/7 P(BANK2) = 2/7
‚óº Disambiguation: ‚ÄúI lost my left shoe on the banks of the river Nile.‚Äù
‚ùë Score(BANK1)=log(5/7) + log(P(shoe|BANK1))+log(P(on|BANK1))+log(P(the|BANK1)) ‚Ä¶
‚ùë Score(BANK2)=log(2/7) + log(P(shoe|BANK2)+log(P(on|BANK2))+log(P(the|BANK2)) ‚Ä¶
BANK1 BANK2
90
Example (with add 0.5 smoothing)
‚óº Assume V = 50
‚ùë P(the|BANK1) = (5+.5) / (30+.5V) P(the|BANK2) = (3+.5) / (12 + .5V)
‚ùë P(world|BANK1) = (1+.5) / 55 P(world|BANK2) = (0+.5) / 37
‚ùë P(and|BANK1) = (1+.5) / 55 P(and|BANK2) = (0+.5) /
‚ùë ‚Ä¶
‚ùë P(off|BANK1) = (0+.5) / 55 P(off|BANK2) =
(1+.5) / 37
‚ùë P(Potomac|BANK1) = (0+.5) / 55 P(Potomac|BANK2) = (1+.5) /
‚ùë P(BANK1) = 5/7
P(BANK2) = 2/7
‚ùë Score(BANK2)=log(2/7) + log(P(shoe|BANK2))+log(P(on|BANK2))+log(P(the|BANK2)) ‚Ä¶
91
Stages of NL Understanding
92
Compositional Semantics
‚óº The cat eats the mouse = The mouse is eaten by the cat.
‚óº Goal:
‚ùë map an expression into a knowledge representation
‚óº a representation of context-independent, literal meaning
‚óº e.g. first-order predicate logic, conceptual graph, embedding...
‚ùë to assign semantic roles (different from grammatical roles):
‚óº Semantic roles: Agent, Patient, Instrument, Time,  Location, ‚Ä¶
‚óº Grammatical roles:  subject, direct object, ...
‚óº E.g.
‚ùë The child hid the candy under the bed.
Hide (agent=child, patient=candy,
location=under_the_bed, time=past)
93
Some Difficulties
‚óº Syntax is not enough
‚ùë I ate spaghetti with a fork.  <instrument>
‚ùë I ate spaghetti with my sister.     <accompanying person>
‚ùë I ate spaghetti with meat balls.  <attribute of food>
‚ùë I ate spaghetti with lots of appetite. <manner>
‚ùë Gun = instrument that can kill
‚ùë Metal gun‚Ä¶ a gun made out of metal
‚ùë Water gun‚Ä¶ a gun made out of water?
‚ùë Fake gun‚Ä¶ it is a gun anyways?  Can it kill?
‚ùë General Kane‚Ä¶ person     but  General Motors ‚Ä¶ corporation
‚óº Parallel problems to syntactic ambiguity
‚ùë Happy [cats and dogs] live on the farm
‚ùë [Happy cats] and dogs live on the farm
‚óº Quantifier Scoping
‚ùë Every man loves a woman.
94
95
Discourse Analysis
‚óº In logics:
‚óº Not in NL:
‚ùë John visited Paris.  He bought Mary some expensive
perfume.  Then he flew home.  He went to Walmart.  He
bought some underwear.
‚ùë John visited Paris. Then he flew home. He went to
Walmart. He bought Mary some expensive perfume. He
‚óº Humans infer relations between sentences that may not
be explicitly stated in order to make a text coherent.
‚ùë (?) I am going to Concordia.  I need butter.
96
Examples of Discourse Relations
CONDITION  If it rains, I will go out.
SEQUENCE Do this, then do that.
CONTRAST This is good, but this is better.
CAUSE Because I was sick, I could not do my
assignment.
RESULT  Click on the button, the red light will
blink.
PURPOSE To use the computer, get an access
code.
ELABORATION The solution was developed by Alan Turing.
Turing was a great
mathematician living in
Great Britain. He was an
atheist as well as gay.
Another Classification Problem, again!
‚óº Discourse tagging can be viewed as typical classification problem
‚óº use machine learning techniques (ex. Na√Øve Bayes classifier, decision
tree) to train a system
‚óº that learns a classifier to assign to unseen sentences one of a fixed
number of discourse relations (categories)
‚óº Sentence  Ex. If it rains, I will go out.
‚óº Features?
‚ñ™ Connectives such as ‚Äúif‚Äù, ‚Äúhowever‚Äù, ‚Äúin conclusion‚Äù
‚ñ™ Tense of verb (future, past)
‚ñ™ ‚Ä¶
‚óº Most likely relation in the sentence (none, condition, contrast,
purpose, ‚Ä¶)
98
99
Pragmatics
‚óº goes beyond the literal meaning of a sentence
‚óº tries to explain what the speaker is really expressing
‚óº understanding how people use language socially
‚ùë E.g.: figures of speech, ‚Ä¶
‚ùë E.g.: Could you spare some change?
100
101
Using World Knowledge
‚óº Using our general knowledge of the world to interpret
a sentence/discourse
The trophy would not fit in the brown suitcase because ...
... it was too big.
... it was too small.
The professor sent the student to see the principal because‚Ä¶
‚Ä¶he wanted to see him.
‚Ä¶he was throwing paper balls in class.
‚Ä¶he could not take it anymore.
‚ùë Ex: Silence of the lambs‚Ä¶
Current Research area: see Winograd Schema Challenge
https://www.youtube.com/watch?v=sbJ89LFheTs
https://en.wikipedia.org/wiki/Winograd_Schema_Challenge
102
Summary of NLU
World Knowledge
Recap
103
104
(to see in a few classes)
